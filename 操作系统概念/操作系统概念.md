# 操作系统概念

## 2.Operating-System Structures

> An operating system provides the environment within which programs are executed. 

- To describe the services an operating system provides to users, processes, and other systems.
- To discuss the various ways of structuring an operating system.
- To explain how operating systems are installed and customized and how they boot.

***

### 2.1 Operating-System Services

>An operating system provides an environment for the execution of programs.
>It provides certain services to programs and to the users of those programs.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201106220804177.png" alt="image-20201106220804177" style="zoom:50%;" />

> One set of operating system services provides functions that are helpful to the user.
>
> **User interface.** Almost all operating systems have a user interface (UI). This interface can take several forms. 
>
> - One is a **command-line interface** (CLI), which uses text commands and a method for entering them (say, a keyboard for typing in commands in a specific format with specific options).
> - Another is a **batch interface**, in which commands and directives to control those commands are entered into files, and those files are executed.
> - Most commonly, a **graphical user interface (GUI)** is used.
>
> **Program execution.**
>
> - The system must be able to load a program into memory and to run that program. The program must be able to end its execution, either normally or abnormally.
>
> **I/O operations.** 
>
> - A running program may require I/O, which may involve a file or an I/O device. 
>
> **File-system manipulation.** 
>
> - The file system is of particular interest. Obviously, programs need to read and write files and directories.
>
> **Communications.** 
>
> - There are many circumstances in which one process needs to exchange information with another process. 
>   - Communications may be implemented via **shared memory**, in which two or more processes read and write to a shared section of memory,
>   - or **message passing**, in which packets of information in predefined formats are moved between processes by the operating system.
>
> **Error detection.** 
>
> - The operating system needs to be detecting and correcting errors constantly. 
>
> ***
>
> Another set of operating system functions exists not for helping the user but rather for ensuring the efficient operation of the system itself. 
>
> **Resource allocation.**
>
> - When there are multiple users or multiple jobs running at the same time, resources must be allocated to each of them. 
>
> **Accounting.** 
>
> - We want to keep track of which users use how much and what kinds of computer resources. 
>
> **Protection and security.** 
>
> - The owners of information stored in a multiuser or networked computer system may want to control use of that information.

***

### 2.2 User and Operating-System Interface

**Command Interpreters**

> Some operating systems include the command interpreter in the kernel.
>
> Others, such as Windows and UNIX, treat the command interpreter as a special program that is running when a job is initiated or when a user first logs on.

On systems with multiple command interpreters to choose from, the interpreters are known as **shells**. 

- on UNIX and Linux systems, a user may choose among several different shells, including the *Bourne shell, C shell, Bourne-Again shell, Korn shell,* and others. 

- The main function of the command interpreter is to get and execute the next user-specified command. 
  - Many of the commands given at this level manipulate files: *create, delete, list, print, copy, execute, and so on*. These commands can be implemented in two general ways.
    - In one approach, the command interpreter itself contains the code to execute the command. For example, a command to delete a file may cause the command interpreter to *jump to a section of its code that sets up the parameters and makes the appropriate system call.* 
    - An alternative approach—used by UNIX, among other operating systems—implements most commands through system programs. In this case, the command interpreter merely uses the command to *identify a file to be loaded into memory and executed*.

***

**Graphical User Interfaces**

> Users employ a mouse-based window- and-menu system characterized by a desktop metaphor. 
>
> The user moves the mouse to position its pointer on images, or icons, on the screen (the desktop)
> that represent programs, files, directories, and system functions.
>
> Depending on the mouse pointer’s location, clicking a button on the mouse can invoke a program, select a file or directory—known as a folder—or pull down a menu that contains commands.

***

**Choice of Interface**

> if a frequent task requires a set of command-line steps, those steps can be recorded into a file, and that file can be run just like a program. The program is not compiled into executable code but rather is interpreted by the command-line interface. These shell scripts are very common on systems that are command-line oriented.

***

### 2.3 System Calls

>System calls provide an interface to the services made available by an operating system. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201107100459021.png" alt="image-20201107100459021" style="zoom:50%;" />

>A programmer accesses an API via a library of code provided by the operating system. In the case of UNIX and Linux for programs written in the C language, the library is called **libc**.

***

### 2.4 Types of System Calls

>System calls can be grouped roughly into six major categories: **process control**, **file manipulation**, **device manipulation**, **information maintenance**, **communications**, and **protection**.

**Process Control**

>A running program needs to be able to halt its execution either normally (end()) or abnormally (abort()). 

More severe errors can be indicated by a higher-level error parameter. 

- **Process control**
  ◦ end, abort
  ◦ load, execute
  ◦ create process, terminate process
  ◦ get process attributes, set process attributes
  ◦ wait for time
  ◦ wait event, signal event
  ◦ allocate and free memory
- **File management**
  ◦ create file, delete file
  ◦ open, close
  ◦ read, write, reposition
  ◦ get file attributes, set file attributes
- **Device management**
  ◦ request device, release device
  ◦ read, write, reposition
  ◦ get device attributes, set device attributes
  ◦ logically attach or detach devices
- **Information maintenance**
  ◦ get time or date, set time or date
  ◦ get system data, set system data
  ◦ get process, file, or device attributes
  ◦ set process, file, or device attributes
- **Communications**
  ◦ create, delete communication connection
  ◦ send, receive messages
  ◦ transfer status information
  ◦ attach or detach remote devices

>Quite often, two or more processes may share data. To ensure the integrity of the data being shared, operating systems often provide system calls allowing a process to **lock** shared data. 
>
>Then, no other process can access the data until the lock is released. 

***

**File Management**

>We first need to be able to **create()** and **delete()** files. Either system call requires the name of the file and perhaps some of the file’s attributes. Once the file is created, we need to **open()** it and to use it. We may also **read()**, **write()**, or **reposition()** (rewind or skip to the end of the file, for example). Finally, we need to close() the file, indicating that we are no longer using it.

***

**Device Management**

>A process may need several resources to execute—main memory, disk drives, access to files, and so on. If the resources are available, they can be granted, and control can be returned to the user process. Otherwise, the process will have to wait until sufficient resources are available.
>
>The various resources controlled by the operating system can be thought of as devices. 
>
>A system with multiple users may require us to first **request()** a device, to ensure exclusive use of it. After we are finished with the device, we **release()** it. 
>
>Once the device has been requested (and allocated to us), we can read(), write(), and (possibly) reposition() the device, just as we can with files. 

***

**Information Maintenance**

>Many systems provide system calls to **dump()** memory. This provision is useful for debugging. A program **trace** lists each system call as it is executed. Even microprocessors provide a CPU mode known as **single step**, in which a trap is executed by the CPU after every instruction. The trap is usually caught by a debugger.

***

**Communication**

>There are two common models of interprocess communication: the **message-passing model** and the **shared-memory model**. 
>
>- In the message-passing model
>  - the communicating processes exchange messages with one another to transfer information. Messages can be exchanged between the processes either directly or indirectly through a common mailbox.
>  - Before communication can take place, a connection must be opened. The name of the other communicator must be known, be it another process on the same system or a process on another computer connected by a communications network. 
>  - Each process has a **process name**, and this name is translated into an identifier by which the operating system can refer to the process.
>
>Most processes that will be receiving connections are special-purpose **daemons**, which are system programs provided for that purpose. 
>
>***
>
>- In the shared-memory model
>  - processes use shared_memory_create() and shared_memory_attach() system calls to create and gain access to regions of memory owned by other processes. 

***

**Protection**

>Protection provides a mechanism for controlling access to the resources provided by a computer system. 

***

### 2.5 System Programs

> System programs, also known as **system utilities**, provide a convenient environment for program development and execution.
>
> Some of them are simply user interfaces to system calls. Others are considerably more complex. They can be divided into these categories:
>
> - **File management**. These programs create, delete, copy, rename, print, dump, list, and generally manipulate files and directories.
> - **Status information.** Some programs simply ask the system for the date, time, amount of available memory or disk space, number of users, or similar status information. Others are more complex, providing detailed performance, logging, and debugging information.
> - **File modification.** Several text editors may be available to create and modify the content of files stored on disk or other storage devices. 
> - **Programming-language support**. Compilers, assemblers, debuggers, and interpreters for common programming languages 
> - **Program loading and execution.** Once a program is assembled or compiled, it must be loaded into memory to be executed.
> - **Communications.** These programs provide the mechanism for creating virtual connections among processes, users, and computer systems. 
> - **Background services.** All general-purpose systems have methods for launching certain system-program processes at boot time. Constantly running system-program processes are known as **services**, **subsystems**, or **daemons.** 

***

### 2.6 Operating-System Design and Implementation

**Design Goals**

> At the highest level, the design of the system will be affected by the choice of hardware and the type of system: batch, time sharing, single user, multiuser, distributed, real time, or general purpose.
>
> The requirements can, however, be divided into two basic groups: **user goals** and **system goals**.

**Mechanisms and Policies**

> One important principle is the separation of **policy** from **mechanism**.
>
> Mechanisms determine ***how*** to do something; policies determine ***what*** will be done.
>
> The separation of policy and mechanism is important for flexibility. 
>
> Policies are likely to change across places or over time. In the worst case, each change in policy would require a change in the underlying mechanism. 
>
> A general mechanism insensitive to changes in policy would be more desirable. 

**Implementation**

***

### 2.7 Operating-System Structure

**question: how the probes work???**

>A system as large and complex as a modern operating system must be engineered carefully if it is to function properly and be modified easily. A common approach is to partition the task into small components, or modules, rather than have one monolithic system. 

**Layered Approach**

>A system can be made modular in many ways. One method is the layered approach, in which the operating system is broken into a number of layers (levels). The bottom layer (layer 0) is the hardware; the highest (layer N) is the user interface.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108095255424.png" alt="image-20201108095255424" style="zoom: 33%;" />

The main advantage of the layered approach is simplicity of construction and debugging. 

The layers are selected so that each uses functions (operations) and services of only lower-level layers. This approach simplifies debugging and system verification. 

>Each layer is implemented only with operations provided by lower-level layers. A layer does not need to know how these operations are implemented; it needs to know only what these operations do.

**Modules**

> Perhaps the best current methodology for operating-system design involves using ***loadable kernel modules***.
>
> Here, the kernel has a set of core components and links in additional services via modules, either at boot time or during run time. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108122746289.png" alt="image-20201108122746289" style="zoom:50%;" />

**Mac OS X**

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108123546257.png" alt="image-20201108123546257" style="zoom:50%;" />

***

### 2.8 Operating-System Debugging

**Failure Analysis**

> If a process fails, most operating systems write the error information to a ***log file*** to alert system operators or users that the problem occurred. 
>
> The operating system can also take a ***core dump***—a capture of the memory of the process—and store it in a file for later analysis. 
>
> A failure in the kernel is called a ***crash***. When a crash occurs, error information is saved to a log file, and the memory state is saved to a ***crash dump***.

- A kernel *failure in the file-system code* would make it risky for the kernel to try to save its state to a file on the file system before rebooting.
  - A common technique is to save the kernel’s memory state to ***a section of disk set aside for this purpose*** that contains no file system. If the kernel detects an unrecoverable error, it writes the entire contents of memory, or at least the kernel-owned parts of the system memory, to the disk area. 
  - Obviously, such strategies would be unnecessary for debugging ordinary user-level processes.

***

**Performance Tuning**

>We mentioned earlier that performance tuning seeks to improve performance by removing processing *bottlenecks*. 
>
>- To identify bottlenecks, we must be able to monitor system performance. 
>  - In a number of systems, the operating system does this by producing ***trace listings*** of system behavior. All interesting events are logged with their time and important parameters and are written to a file. 
>    - Later, an analysis program can process the log file to determine system performance and to identify bottlenecks and inefficiencies. 
>  - Another approach to performance tuning uses single-purpose, interactive tools that allow users and administrators to question the state of various system components to look for bottlenecks. 

***

**DTrace**

>DTrace is a facility that dynamically adds probes to a running system, both in user processes and in the kernel. These probes can be queried via the D programming language to determine an astonishing amount about the kernel, the system state, and process activities. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\T45M@LOT0N%W`[YENP0CZ0.png" alt="img" style="zoom:50%;" />

- In contrast, DTrace runs on production systems—systems that are running important or critical applications—and causes no harm to the system. It slows activities while enabled, but after execution it resets the system to its pre-debugging state.
- DTrace is composed of a compiler, a framework, ***providers*** of probes written within that framework, and ***consumers*** of those probes.

>DTrace providers create probes. 
>
>Kernel structures exist to keep track of all probes that the providers have created. The probes are stored in *a hash-table data structure* that is hashed by name and indexed according to unique probe identifiers.
>
>When a probe is enabled, a bit of code in the area to be probed is rewritten to call dtrace_probe (probe identifier) and then continue with the code’s original operation.
>
>***

> A DTrace consumer is code that is interested in a probe and its results.
>
> A consumer requests that the provider create one or more probes. When a probe fires, it emits data that are managed by the kernel. Within the kernel, actions called ***enabling control blocks***, or ECBs, are performed when probes fire. 
>
> Once the probe consumer terminates, its ECBs are removed. If there are no ECBs consuming a probe, the probe is removed. That involves rewriting the code to remove the dtrace_probe() call and put back the original code. 
>
> Thus, before a probe is created and after it is destroyed, the system is exactly the same, as if no probing occurred.

***

## 3.Processes

### 3.1 Process Concept

**The Process**

> Informally, a process is a program in execution.
>
> A process is more than the program code, which is sometimes known as the ***text section***.
>
> - It also includes the current activity, as represented by the value of the ***program counter*** and the contents of the processor’s registers. 
>
> - A process generally also includes the process ***stack***, which contains temporary data, and a ***data section***, which contains global variables. 
> - A process may also include a ***heap***, which is memory that is dynamically allocated during process run time. 

A program is a **passive** entity, In contrast, a process is an **active** entity, with a program counter specifying the next instruction to execute and a set of associated resources. 

A program becomes a process when an executable file is loaded into memory.

- Two common techniques for loading executable files are 
  - double-clicking an icon representing the executable file 
  - and entering the name of the executable file on the command line. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108224532445.png" alt="image-20201108224532445" style="zoom:33%;" />

>Although two processes may be associated with the same program, they are nevertheless considered two ***separate execution sequences***. 

***

**Process State**

>As a process executes, it changes ***state***. The state of a process is defined in part by the current activity of that process. A process may be in one of the following states:

- **New**. The process is being created.
- **Running**. Instructions are being executed.
- **Waiting**. The process is waiting for some event to occur (such as an I/O completion or reception of a signal).
- **Ready**. The process is waiting to be assigned to a processor.
- **Terminated**. The process has finished execution.

>It is important to realize that only one process can be running on any processor at any instant. Many
>processes may be ready and waiting, however.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108225151561.png" alt="image-20201108225151561" style="zoom:50%;" />

***

**Process Control Block**

>Each process is represented in the operating system by a ***process control block*** (PCB)—also called a ***task control block***. It contains many pieces of information associated with a specific process, including these:

- **Process state.** The state may be new, ready, running, waiting, halted, and so on.
- **Program counter.** The counter indicates the address of the next instruction to be executed for this process.
- **CPU registers.** The registers vary in number and type, depending on the computer architecture.Along with the program counter, this state information must be saved when an interrupt occurs, to allow the process to be continued correctly afterward.
- **CPU-scheduling information.** This information includes a process priority, pointers to scheduling queues, and any other scheduling parameters.
- **Memory-management information.** This information may include such items as the value of the base and limit registers and the page tables, or the segment tables, depending on the memory system used by the operating system.
- **Accounting information.** This information includes the amount of CPU and real time used, time limits, account numbers, job or process numbers, and so on.
- **I/O status information.** This information includes the list of I/O devices allocated to the process, a list of open files, and so on.

>In brief, the PCB simply serves as the repository for any information that may vary from process to process.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108225915128.png" alt="image-20201108225915128" style="zoom:50%;" />

***

**Threads**

>The process model discussed so far has implied that a process is a program that performs a single ***thread*** of execution. 
>
>This single thread of control allows the process to perform only one task at a time.
>
>The user cannot simultaneously type in characters and run the spell checker within the same process.
>
>Most modern operating systems have extended the process concept to allow a process to have multiple threads of execution and thus to perform more than one task at a time.

***

> The process control block in the Linux operating system is represented by the C structure task struct, which is found in the ***<linux/sched.h>*** include file in the kernel source-code directory. This structure contains all the necessary information for representing a process, including the state of the process, scheduling and memory-management information, list of open files, and pointers to the process’s parent and a list of its children and siblings. Some of these fields include:

```c
long state; 					/* state of the process */
struct sched_entity se; 		/* scheduling information */
struct task_struct *parent; 	/* this process’s parent */
struct list_head children; 		/* this process’s children */
struct files_struct *files; 	/* list of open files */
struct mm_struct *mm; 			/* address space of this process */
```

> Within the Linux kernel, all active processes are represented using a doubly linked list of task_struct. The kernel maintains a pointer—current—to the process currently executing on the system, as shown below:

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\8WTO%P}5Q`NQ2S6YDBVBDI.png" alt="img" style="zoom:50%;" />

****

### 3.2 Process Scheduling

>The objective of multiprogramming is to *have some process running at all times*, to *maximize CPU utilization*. The objective of time sharing is to switch the CPU among processes so frequently that users can interact with each program while it is running. 
>
>To meet these objectives, the **process scheduler** selects an available process (possibly from a set of several available processes) for program execution on the CPU.

**Scheduling Queues**

>As processes enter the system, they are put into a ***job queue***, which consists of all processes in the system. 
>
>The processes that are residing in main memory and are ready and waiting to execute are kept on a list called the ***ready queue***.
>
>This queue is generally stored as a *linked list*. A ready-queue header contains pointers to the first and final PCBs in the list. Each PCB includes a pointer field that points to the next PCB in the ready queue.
>
>The list of processes waiting for a particular I/O device is called a ***device queue***. Each device has its own device queue.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\[SW7VU22898OKY1CC@%]7B.png" alt="img" style="zoom:50%;" />

> A new process is initially put in the ready queue. It waits there until it is selected for execution, or **dispatched**. Once the process is allocated the CPU and is executing, one of several events could occur:
>
> - The process could issue an I/O request and then be placed in an I/O queue.
> - The process could create a new child process and wait for the child’s termination.
> - The process could be removed forcibly from the CPU, as a result of an interrupt, and be put back in the ready queue.

**Schedulers**

>The operating system must select, for scheduling purposes, processes from these queues in some fashion. The selection process is carried out by the appropriate **scheduler.**
>
>Often, in a batch system, more processes are submitted than can be executed immediately. These processes are spooled to a ***mass-storage device*** (typically a disk), where they are kept for later execution. 
>
>The **long-term scheduler**, or job scheduler, selects processes from this pool and loads them into memory for execution. 
>
>The **short-term scheduler**, or CPU scheduler, selects from among the processes that are ready to execute and allocates the CPU to one of them.

- The primary distinction between these two schedulers lies in ***frequency*** of execution. 
  - The short-term scheduler must select a new process for the CPU frequently. 
  - A process may execute for only a few milliseconds before waiting for an I/O request. 
  - Because of the short time between executions, the short-term scheduler must be fast. 
- The long-term scheduler executes much less frequently
  - minutes may separate the creation of one new process and the next. 
  - The long-term scheduler controls the **degree of multiprogramming** (the number of processes in memory). 
- In general, most processes can be described as either I/O bound or CPU bound.
  - An **I/O-bound** process is one that spends more of its time doing I/O than it spends doing computations. 
  - A **CPU-bound** process, in contrast, generates I/O requests infrequently, using more of its time doing computations. 
- This **medium-term scheduler** is diagrammed in Figure 3.7. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201113214701206.png" alt="image-20201113214701206" style="zoom:50%;" />

***

**Context Switch**

>When an interrupt occurs, the system needs to save the current ***context*** of the process running on the CPU.
>
>Generically, we perform a ***state save*** of the current state of the CPU, be it in kernel or user mode, and then a ***state restore*** to resume operations.

- Switching the CPU to another process requires performing a state save of the current process and a state restore of a different process. This task is known as a **context switch**. 

***

### 3.3 Operations on Processes

**Process Creation**

> During the course of execution, a process may create several new processes. 
>
> Each of these new processes may in turn create other processes, forming a **tree** of processes.
>
> Most operating systems (including UNIX, Linux, and Windows) identify processes according to a unique process identifier (or ***pid***), which is typically an integer number. 
>
> 

![image-20201113220754861](C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201113220754861.png)

- The ***init*** process (which always has a pid of 1) serves as the ***root parent process*** for all user processes.

  >Once the system has booted, the init process can also create various user processes, such as a web or print server, an ssh server, and the like. 

- We see two children of init—***kthreadd*** and ***sshd***. 

  - The kthreadd process is responsible for creating additional processes that perform tasks on behalf of the kernel (in this situation, khelper and pdflush). 
  - The sshd process is responsible for managing clients that connect to the system by using ssh (which is short for ***secure shell***).

- The ***login*** process is responsible for managing clients that directly log onto the system.

>On UNIX and Linux systems, we can obtain a listing of processes by using the **ps** command.

When a process creates a new process, two possibilities for execution exist:
- The parent continues to execute concurrently with its children.

- The parent waits until some or all of its children have terminated.

There are also two address-space possibilities for the new process:
- The child process is a duplicate of the parent process (it has the same program and data as the parent).

- The child process has a new program loaded into it.

>Both processes (the parent and the child) continue execution at the instruction after the ***fork()***, with one difference: 
>
>the return code for the fork() is zero for the new (child) process, whereas *the (nonzero) process identifier of the child* is returned to the parent.
>
>After a fork() system call, one of the two processes typically uses the ***exec()*** system call to ***replace the process’s memory space with a new program***. The exec() system call *loads a binary file* into memory (destroying the memory image of the program containing the exec() system call) and starts
>its execution. 
>
>The parent can then create more children; or, if it has nothing else to do while the child runs, it can issue a ***wait()*** system call to move itself off the ready queue until the termination of the child. 

```c
#include <sys/types.h>
#include <stdio.h>
#include <unistd.h>
int main()
{
	pid_t pid;
    /* fork a child process */
    pid = fork();
    if (pid < 0) { 		/* error occurred */
        fprintf(stderr, "Fork Failed");
        return 1;
    }
    else if (pid == 0) { /* child process */
    	execlp("/bin/ls","ls",NULL);
    }
    else { /* parent process */
        /* parent will wait for the child to complete */
        wait(NULL);
        printf("Child Complete");
    }
    return 0;
}
```

![image-20201113224305769](C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201113224305769.png)

> The C program shown in Figure 3.11 illustrates the CreateProcess() function, which creates a child process that loads the application mspaint.exe.

```c
#include <stdio.h>
#include <windows.h>
int main(VOID)
{
STARTUPINFO si;
PROCESS_INFORMATION pi;
    /* allocate memory */
    ZeroMemory(&si, sizeof(si));
    si.cb = sizeof(si);
    ZeroMemory(&pi, sizeof(pi));
    
    /* create child process */
    if (!CreateProcess(NULL, /* use command line */
        "C:\\WINDOWS\\system32\\mspaint.exe", /* command */
        NULL, /* don’t inherit process handle */
        NULL, /* don’t inherit thread handle */
        FALSE, /* disable handle inheritance */
        0, /* no creation flags */
        NULL, /* use parent’s environment block */
        NULL, /* use parent’s existing directory */
        &si,
        &pi))
    {
        fprintf(stderr, "Create Process Failed");
        return -1;
    }
    
    /* parent will wait for the child to complete */
    WaitForSingleObject(pi.hProcess, INFINITE);
    printf("Child Complete");
    
    /* close handles */
    CloseHandle(pi.hProcess);
    CloseHandle(pi.hThread);
}
```

***

**Process Termination**

>A process terminates when it finishes executing its final statement and asks the operating system to delete it by using the exit() system call. At that point, the process may return a status value (typically an integer) to its parent process (via the wait() system call).
>
>A process can cause the termination of another process via an appropriate system call.
>
>A parent may *terminate the execution of one of its children* for a variety of reasons, such as these:
>
>- The child has exceeded its usage of some of the resources that it has been allocated. (To determine whether this has occurred, the parent must have a mechanism to inspect the state of its children.)
>
>- The task assigned to the child is no longer required.
>
>- The parent is exiting, and the operating system does not allow a child to continue if its parent terminates. 
>    - This phenomenon, referred to as **cascading termination**, is normally initiated by the operating system.

- A parent process may wait for the termination of a child process by using the wait() system call. 
  - The wait() system call is passed a parameter that allows the parent to obtain the exit status of the child. 
  - This system call also returns the process identifier of the terminated child so that the parent can tell which of its children has terminated:

>Now consider what would happen if a parent did not invoke wait() and instead terminated, thereby leaving its child processes as ***orphans***.

- Linux and UNIX address this scenario by assigning the init process as the new parent to orphan processes. 
- The init process periodically invokes wait(), thereby allowing the exit status of any orphaned process to be collected and releasing the orphan’s process identifier and process-table entry.

****

### 3.4 Interprocess Communication

>A process is ***independent*** if it cannot affect or be affected by the other processes executing in the system.
>
>A process is ***cooperating*** if it can affect or be affected by the other processes executing in the system.

- There are several reasons for providing an environment that allows process cooperation:
  - ***Information sharing.*** Several users may be interested in the same piece of information (for instance, a shared file).
  - ***Computation speedup.*** If we want a particular task to run faster, we must break it into subtasks, each of which will be executing in parallel with the others. 
  - ***Modularity.*** We may want to construct the system in a modular fashion, dividing the system functions into separate processes or threads.
  - ***Convenience.*** Even an individual user may work on many tasks at the same time. 

>Cooperating processes require an ***interprocess communication*** (***IPC***) mechanism that will allow them to exchange data and information.
>
>There are two fundamental models of interprocess communication: 
>
>**shared memory** and **message passing**.
>
>- In the shared-memory model, a region of memory that is shared by cooperating processes is established. 
>  - Processes can then exchange information by reading and writing data to the shared region. 
>- In the message-passing model, communication takes place by means of messages exchanged between the cooperating processes. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201115102351001.png" alt="image-20201115102351001" style="zoom:50%;" />

***

**Shared-Memory Systems**

>To illustrate the concept of cooperating processes, let’s consider the producer–consumer problem.
>
>A producer process produces information that is consumed by a consumer process. For example, a compiler may produce assembly code that is consumed by an assembler. 
>
>One solution to the producer–consumer problem uses shared memory. 
>
>- To allow producer and consumer processes to run concurrently, we must haveavailable a buffer of items that can be filled by the producer and emptied by the consumer. 
>- This buffer will reside in a region of memory that is shared by the producer and consumer processes.
>- Two types of buffers can be used. 
>  - The ***unbounded buffer*** places no practical limit on the size of the buffer. The consumer may have to wait for new items, but the producer can always produce new items. 
>  - The ***bounded buffer*** assumes a fixed buffer size. In this case, the consumer must wait if the buffer is empty, and the producer must wait if the buffer is full.

****

**Message-Passing Systems**

> Message passing may be either ***blocking*** or ***nonblocking***— also known as ***synchronous*** and ***asynchronous***. 
>
> - **Blocking send**. The sending process is blocked until the message is received by the receiving process or by the mailbox.
> - **Nonblocking send.** The sending process sends the message and resumes operation.
> - **Blocking receive.** The receiver blocks until a message is available.
> - **Nonblocking receive.** The receiver retrieves either a valid message or a null.

***

### 3.5 Examples of IPC Systems

**An Example: POSIX Shared Memory**

>POSIX shared memory is organized using ***memory-mapped files***, which associate the region of shared memory with a file. A process must first create a shared-memory object using the ***shm_open()*** system call, as follows:
>
>```c
>shm_fd = shm_open(name, O_CREAT | O_RDRW, 0666);
>```
>
>- The first parameter specifies the ***name*** of the shared-memory object. Processes that wish to access this shared memory must refer to the object by this name.
>- The subsequent parameters specify that the shared-memory object is to be created if it does not yet exist (O_CREAT) and that the object is open for reading and writing (O_RDRW). 
>- The last parameter establishes the directory ***permissions*** of the shared-memory object. 
>- A successful call to shm_open() returns an ***integer file descriptor*** for the shared-memory object.

- Once the object is established, the ***ftruncate()*** function is used to configure the size of the object in bytes. 

```c
ftruncate(shm_fd, 4096);
```

- Finally, the ***mmap()*** function establishes a memory-mapped file containing the shared-memory object. It also returns a pointer to the memory-mapped file that is used for accessing the shared-memory object.

```c
#include <stdio.h>
#include <stlib.h>
#include <string.h>
#include <fcntl.h>
#include <sys/shm.h>
#include <sys/stat.h>
int main()
{
    /* the size (in bytes) of shared memory object */
    const int SIZE 4096;
    /* name of the shared memory object */
    const char *name = "OS";
    /* strings written to shared memory */
    const char *message_0 = "Hello";
    const char *message_1 = "World!";
    /* shared memory file descriptor */
    int shm_fd;
    /* pointer to shared memory obect */
    void *ptr;
    /* create the shared memory object */
    shm_fd = shm_open(name, O_CREAT | O_RDRW, 0666);
    /* configure the size of the shared memory object */
    ftruncate(shm_fd, SIZE);
    /* memory map the shared memory object */
    ptr = mmap(0, SIZE, PROT_WRITE, MAP_SHARED, shm_fd, 0);
    /* write to the shared memory object */
    sprintf(ptr,"%s",message_0);
    ptr += strlen(message_0);
    sprintf(ptr,"%s",message_1);
    ptr += strlen(message_1);
    return 0;
}
```

```c
#include <stdio.h>
#include <stlib.h>
#include <fcntl.h>
#include <sys/shm.h>
#include <sys/stat.h>
int main()
{
    /* the size (in bytes) of shared memory object */
    const int SIZE 4096;
    /* name of the shared memory object */
    const char *name = "OS";
    /* shared memory file descriptor */
    int shm_fd;
    /* pointer to shared memory obect */
    void *ptr;
    /* open the shared memory object */
    shm_fd = shm_open(name, O_RDONLY, 0666);
    /* memory map the shared memory object */
    ptr = mmap(0, SIZE, PROT_READ, MAP_SHARED, shm_fd, 0);
    /* read from the shared memory object */
    printf("%s",(char *)ptr);
    /* remove the shared memory object */
    shm_unlink(name);
    return 0;
}
```

>Even system calls are made by messages. When a task is created, two special mailboxes—the Kernel mailbox and the Notify mailbox—are also created.
>
>The kernel uses the Kernel mailbox to communicate with the task and sends notification of event occurrences to the Notify port. Only three system calls are needed for message transfer. 
>
>- The msg_send() call sends a message to a mailbox. 
>
>- A message is received via msg_receive(). 
>- Remote procedure calls (RPCs) are executed via msg_rpc(), which sends a message and waits for exactly one return message from the sender. 

***

**An Example: Windows**

> The message-passing facility in Windows is called the ***advanced local procedure call*** (ALPC) facility. 
>
> Windows uses a port object to establish and maintain a connection between two processes. Windows uses two types of ports: ***connection ports*** and ***communication ports.***
>
> - Server processes publish connection-port objects that are visible to all processes. 
>   - When a client wants services from a subsystem, it opens a handle to the server’s connection-port object and sends a connection request to that port.
>   - The server then creates a channel and returns a handle to the client. 
>   - The channel consists of a pair of private communication ports: 
>     - one for client—server messages,
>     - the other for server—client messages.
>   - Additionally, communication channels support a ***callback mechanism*** that allows the client and server to accept requests when they would normally be expecting a reply.
> - When an ALPC channel is created, one of three message-passing techniques is chosen:
>   1. For small messages (up to 256 bytes), the port’s message queue is used as intermediate storage, and the messages are copied from one process to the other.
>   2. Larger messages must be passed through a section object, which is a region of shared memory associated with the channel.
>   3. When the amount of data is too large to fit into a section object, an API is available that allows server processes to read and write directly into the address space of a client.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116102604656.png" alt="image-20201116102604656" style="zoom:50%;" />

***

### 3.6 Communication in Client–Server Systems

**Sockets**

> A socket is defined as an endpoint for communication. 

**Remote Procedure Calls**

> A port is simply a number included at the start of a message packet.

- The RPC system hides the details that allow communication to take place by providing a **stub** on the client side.

  >A separate stub exists for each separate remote procedure. 
  >
  >When the client invokes a remote procedure, the RPC system calls the appropriate stub, passing it the parameters provided to the remote procedure. 
  >
  >This stub locates the port on the server and **marshals** the parameters. Parameter marshalling involves packaging the parameters into a form that can be transmitted over a network. 
  >
  >The stub then transmits a message to the server using message passing. 
  >
  >A similar stub on the server side receives this message and invokes the procedure on the server.

- One issue that must be dealt with concerns differences in data representation on the client and server machines. 
  
  - To resolve differences like this, many RPC systems define a machine-independent representation of data. One such representation is known as ***external data representation*** (**XDR**). 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\LHONPZ07RE4}Y5F49Y6_$UD.png" alt="img" style="zoom:50%;" />

****

**Pipes**

> A pipe acts as a conduit allowing two processes to communicate. 
>
> In implementing a pipe, four issues must be considered:	
>
> 1. Does the pipe allow ***bidirectional communication***, or is communication unidirectional?
> 2. If two-way communication is allowed, is it ***half duplex*** (data can travel only one way at a time) or ***full duplex*** (data can travel in both directions at the same time)?
> 3. Must a ***relationship*** (such as parent–child) exist between the communicating processes?
> 4. Can the pipes ***communicate over*** a network, or must the communicating processes reside on the same machine?

**Ordinary Pipes**

>Ordinary pipes allow two processes to communicate in standard producer–consumer fashion: 
>
>the producer writes to one end of the pipe (the **write-end**) and the consumer reads from the other end (the **read-end**). 

- As a result, ordinary pipes are unidirectional, allowing only one-way communication.

- On UNIX systems, ordinary pipes are constructed using the function

  ```c
  pipe(int fd[]);
  ```

  - This function creates a pipe that is accessed through the **int fd[]** file descriptors: 
  - fd[0] is the read-end of the pipe, and fd[1] is the write-end.
  - UNIX treats a pipe as a special type of file. Thus, pipes can be accessed using ordinary read() and write() system calls.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116111311170.png" alt="image-20201116111311170" style="zoom:50%;" />

>Note that ordinary pipes require a parent–child relationship between the communicating processes on both UNIX and Windows systems. This means that these pipes can be used only for communication between processes on the same machine.

**Named Pipes**



****

## 4.Thread

### 4.1 Overview

>A thread is a basic unit of CPU utilization; 
>
>it comprises a thread ID, a program counter, a register set, and a stack. 
>
>It shares with other threads belonging to the same process its code section, data section, and other operating-system resources, such as open files and signals. 
>
>A traditional (or heavyweight) process has a single thread of control. If a process has multiple threads of control, it can perform more than one task at a time. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116123416523.png" alt="image-20201116123416523" style="zoom:50%;" />

**Motivation**

>Most software applications that run on modern computers are multithreaded.
>An application typically is implemented as a separate process with several threads of control. 

***

### 4.2 Multicore Programming

**Programming Challenges**

> The trend towards multicore systems continues to place pressure on system designers and application programmers to make better use of the multiple computing cores. 
>
> For application programmers, the challenge is to modify existing programs as well as design new programs that are multithreaded.
>
> In general, five areas present challenges in programming for multicore systems:
>
> - **Identifying tasks.** This involves examining applications to find areas that can be divided into separate, concurrent tasks. Ideally, tasks are independent of one another and thus can run in parallel on individual cores.
> - **Balance.** Programmers must also ensure that the tasks perform equal work of equal value. Using a separate execution core to run that task may not be worth the cost.
> - **Data splitting.** Just as applications are divided into separate tasks, the data accessed and manipulated by the tasks must be divided to run on separate cores.
> - **Data dependency**. The data accessed by the tasks must be examined for dependencies between two or more tasks. 
> - **Testing and debugging.** When a program is running in parallel on multiple cores, many different execution paths are possible. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116145200668.png" alt="image-20201116145200668" style="zoom:50%;" />

***

**Types of Parallelism**

- In general, there are two types of parallelism: data parallelism and task parallelism. 

  - **Data parallelism** focuses on distributing subsets of the same data across multiple computing cores and performing the same operation on each core.

    >On a dual-core system, however, thread A, running on core 0, could sum the elements [0] . . . [N/2 − 1] while thread B, running on core 1, could sum the elements [N/2] . . . [N − 1]. The two threads would be running in parallel on separate computing cores.

  - **Task parallelism** involves distributing not data but tasks (threads) across multiple computing cores. Each thread is performing a unique operation. Different threads may be operating on the same data, or they may be operating on different data. 

- In most instances, applications use a hybrid of these two strategies.

****

### 4.3 Multithreading Models

>Support for threads may be provided either at the user level, for **user threads**, or by the kernel, for **kernel threads**. 
>
>User threads are supported above the kernel and are managed without kernel support, whereas kernel threads are supported and managed directly by the operating system. 
>
>A relationship must exist between user threads and kernel threads. In this section, we look at three common ways of establishing such a relationship: 
>
>the many-to-one model, the one-to-one model, and the many-to-many model.

****

**Many-to-One Model**

>The many-to-one model (Figure 4.5) maps many user-level threads to one kernel thread. 
>
>Thread management is done by the thread library in user space, so it is efficient.
>
>However, the entire process will block if a thread makes a blocking system call. 
>
>Also, because only one thread can access the kernel at a time, multiple threads are unable to run in parallel on multicore systems.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116152547846.png" alt="image-20201116152547846" style="zoom:50%;" />

**One-to-One Model**

>The one-to-one model (Figure 4.6) maps each user thread to a kernel thread. 
>
>It provides more concurrency than the many-to-one model by allowing another thread to run when a thread makes a blocking system call. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116152618042.png" alt="image-20201116152618042" style="zoom:50%;" />

**Many-to-Many Model**

>The many-to-many model (Figure 4.7) multiplexes many user-level threads to a smaller or equal number of kernel threads.
>
>The number of kernel threads may be specific to either a particular application or a particular machine.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116152921597.png" alt="image-20201116152921597" style="zoom:50%;" />

***

### 4.4 Thread Libraries

>A **thread library** provides the programmer with an API for creating and managing threads. 
>
>There are two primary ways of implementing a thread library. 
>
>- The first approach is to provide a library entirely in user space with no kernel support. 
>  - This means that invoking a function in the library results in a local function call in user space and not a system call.
>- The second approach is to implement a kernel-level library supported directly by the operating system. 

- Three main thread libraries are in use today: POSIX Pthreads, Windows, and Java. 
  - **Pthreads**, the threads extension of the POSIX standard, may be provided as either a user-level or a kernel-level library. 
    - UNIX and Linux systems often use Pthreads.
  - The **Windows thread** library is a kernel-level library available on Windows systems. 
  - The **Java thread** API allows threads to be created and managed directly in Java programs. 
    - Because in most instances the JVM is running on top of a host operating system, the Java thread API is generally implemented using a thread library available on the host system. 

___

- For POSIX and Windows threading, any data ***declared globally***—that is, declared ***outside of any function***—are shared among all ***threads belonging to the same process***. 
- Because Java has no notion of global data, access to shared data must be explicitly arranged between threads. 
  - Data declared local to a function are typically stored on the stack. Since each thread has its own stack, each thread has its own copy of local data.
- There are two general strategies for creating multiple threads: **asynchronous threading** and **synchronous threading**.
  - With asynchronous threading, once the parent creates a child thread, the parent resumes its execution, so that the parent and child execute concurrently. 
  - Synchronous threading occurs when the parent thread creates one or more children and then must wait for all of its children to terminate before it resumes—the so-called **fork-join** strategy. 
  - Once each thread has finished its work, it terminates and joins with its parent. 
  - Only after all of the children have joined can the parent resume execution. 

***

**Pthreads**

>Pthreads refers to the POSIX standard (IEEE 1003.1c) defining an API for thread creation and synchronization. This is a specification for thread behavior, not an implementation. 

The C program shown below demonstrates the basic Pthreads API for constructing a multithreaded program that calculates the summation of a non-negative integer in a separate thread. 

```c
#include <pthread.h>
#include <stdio.h>
int sum; /* this data is shared by the thread(s) */
void *runner(void *param); /* threads call this function */
int main(int argc, char *argv[])
{
    pthread_t tid; /* the thread identifier */
    pthread_attr_t attr; /* set of thread attributes */
    if (argc != 2) {
        fprintf(stderr,"usage: a.out <integer value>\n");
        return -1;
    }
    if (atoi(argv[1]) < 0) {
        fprintf(stderr,"%d must be >= 0\n",atoi(argv[1]));
        return -1;
    }
    /* get the default attributes */
    pthread_attr_init(&attr);
    /* create the thread */
    pthread_create(&tid, &attr, runner, argv[1]);
    /* wait for the thread to exit */
    pthread_join(tid,NULL);
    printf("sum = %d\n",sum);
}

/* The thread will begin control in this function */
void *runner(void *param)
{
    int i, upper = atoi(param);
    sum = 0;
    for (i = 1; i <= upper; i++)
        sum += i;
    pthread_exit(0);
}
```

- In a Pthreads program, separate threads begin execution in a specified function. 
  - In code aobve, this is the runner() function. 
  - When this program begins, a single thread of control begins in main(). 
  - After some initialization, main() creates a second thread that begins control in the runner() function. Both threads share the global data sum.
- All Pthreads programs must include the pthread.h header file. The statement pthread_t tid declares the identifier for the thread we will create. 
- Each thread has a set of attributes, including stack size and scheduling information. The pthread_attr_t attr declaration represents the attributes for the thread. 
  - Because we did not explicitly set any attributes, we use the default attributes provided.
- In addition to passing the thread identifier and the attributes for the thread, we also pass the name of the function where the new thread will begin execution—in this case, the runner() function.

- Last, we pass the integer parameter that was provided on the command line, argv[1].

>This program follows the **fork-join** strategy described earlier: after creating the summation thread, the parent thread will wait for it to terminate by calling the pthread_join() function. 
>
>The summation thread will terminate when it calls the function pthread_exit().

****

**Windows Threads**

````c
#include <windows.h>
#include <stdio.h>
DWORD Sum; /* data is shared by the thread(s) */
/* the thread runs in this separate function */
DWORD WINAPI Summation(LPVOID Param)
{
    DWORD Upper = *(DWORD*)Param;
    for (DWORD i = 0; i <= Upper; i++)
    	Sum += i;
    return 0;
}
int main(int argc, char *argv[])
{
    DWORD ThreadId;
    HANDLE ThreadHandle;
    int Param;
    if (argc != 2) {
        fprintf(stderr,"An integer parameter is required\n");
        return -1;
    }
    Param = atoi(argv[1]);
    if (Param < 0) {
        fprintf(stderr,"An integer >= 0 is required\n");
        return -1;
    }
    
    /* create the thread */
    ThreadHandle = CreateThread(
        NULL, 		/* default security attributes */
        0, 			/* default stack size */
        Summation, 	/* thread function */
        &Param, 	/* parameter to thread function */
        0, 			/* default creation flags */
        &ThreadId); /* returns the thread identifier */
    if (ThreadHandle != NULL) {
        /* now wait for the thread to finish */
        WaitForSingleObject(ThreadHandle,INFINITE);
        /* close the thread handle */
        CloseHandle(ThreadHandle);
        printf("sum = %d\n",Sum);
    }
}
````

***

**Java Threads**



***

### 4.5 Implicit Threading

> With the continued growth of multicore processing, applications containing hundreds—or even thousands—of threads are looming on the horizon.
> Designing such applications is not a trivial undertaking.
>
> One way to address these difficulties and better support the design of multithreaded applications is to transfer the creation and management of threading from application developers to compilers and run-time libraries. 
>
> This strategy, termed **implicit threading**, is a popular trend today. 

**Thread Pools**

Consider this situation:

- Whenever the server receives a request, it creates a separate thread to service the request. 
- The ***first issue*** concerns the amount of **time** required to create the thread, together with the fact that the thread will be discarded once it has completed its work. 
- The ***second issue*** is more troublesome. If we allow all concurrent requests to be serviced in a new thread, we have not placed a **bound** on the number of threads concurrently active in the system. 

>Unlimited threads could exhaust system resources, such as CPU time or memory. One solution to this problem is to use a ***thread pool***.
>
>The general idea behind a thread pool is to create a number of threads at process **startup** and place them into a pool, where they sit and wait for work.

- Thread pools offer these ***benefits***:
  - Servicing a request with an existing thread is ***faster*** than waiting to create a thread.
  - A thread pool ***limits*** the number of threads that exist at any one point. This is particularly important on systems that cannot support a large number of concurrent threads.
  - ***Separating*** the task to be performed from the mechanics of creating the task allows us to use different strategies for running the task. 

- The Windows API provides several functions related to thread pools. 
  - Here, a function that is to run as a separate thread is defined. 
  - A pointer to PoolFunction() is passed to one of the functions in the thread pool API
  - and a thread from the pool executes this function.
  - One such member in the thread pool API is the QueueUserWorkItem() function, which is passed
    three parameters:
    - LPTHREAD_START_ROUTINE Function—a pointer to the function that is to run as a separate thread.
    - PVOID Param—the parameter passed to Function.
    - ULONG Flags—flags indicating how the thread pool is to create and manage execution of the thread.

```c
DWORD WINAPI PoolFunction(AVOID Param) {
    /*
    * this function runs as a separate thread.
    */
}
```

- An example of invoking a function is the following:

  ```c
  QueueUserWorkItem(&PoolFunction, NULL, 0);
  ```

  This causes a thread from the thread pool to invoke PoolFunction() on behalf of the programmer. In this instance, we pass no parameters to PoolFunction(). 

  Because we specify 0 as a flag, we provide the thread pool with no special instructions for thread creation.

***

**OpenMP**

>OpenMP is a set of compiler directives as well as an API for programs written in C, C++, or FORTRAN that provides support for ***parallel programming*** in shared-memory environments. 
>
>OpenMP identifies **parallel regions** as blocks of code that may run in parallel. 
>
>Application developers insert compiler directives into their code at parallel regions, and these directives instruct the OpenMP run-time library to execute the region in parallel. 

- The following C program illustrates a compiler directive above the parallel region containing the printf() statement:

```c
#include <omp.h>
#include <stdio.h>
int main(int argc, char *argv[])
{
    /* sequential code */
    #pragma omp parallel
    {
    	printf("I am a parallel region.");
    }
    /* sequential code */
    return 0;
}
```

- When OpenMP encounters the directive
  `#pragma omp parallel`

  - it creates ***as many threads are there are processing cores*** in the system. Thus, for a dual-core system, two threads are created, for a quad-core system, four are created; and so forth. 

  - All the threads then simultaneously execute the parallel region. As each thread exits the parallel region, it is terminated.

***

**Grand Central Dispatch**





***

### 4.6 Threading Issues

>In this section, we discuss some of the issues to consider in designing multithreaded programs.

**The fork() and exec() System Calls**

>The semantics of the fork() and exec() system calls change in a multithreaded program.

***

### 4.7 Operating-System Examples

**Windows Threads**

> A Windows application runs as a separate process, and each process may contain one or more threads. 

- The general components of a thread include:
  - A ***thread ID*** uniquely identifying the thread
  - A ***register set*** representing the status of the processor
  - A ***user stack***, employed when the thread is running in user mode, and a kernel stack, employed when the thread is running in kernel mode
  - A ***private storage area*** used by various run-time libraries and dynamic link libraries (DLLs)

- The register set, stacks, and private storage area are known as the **context** of the thread.
- The primary data structures of a thread include:
  - ETHREAD—executive thread block
  - KTHREAD—kernel thread block
  - TEB—thread environment block

>The key components of the ETHREAD include a pointer to the process to which the thread belongs and the address of the routine in which the thread starts control. 
>
>The ETHREAD also contains a pointer to the corresponding KTHREAD.
>
>The KTHREAD includes scheduling and synchronization information for the thread. In addition, the KTHREAD includes the kernel stack (used when the thread is running in kernel mode) and a pointer to the TEB.
>
>The ETHREAD and the KTHREAD exist entirely in kernel space; this means that only the kernel can access them. 
>
>The TEB is a user-space data structure that is accessed when the thread is running in user mode. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\1.png" style="zoom:50%;" />

***

## 5.Process Synchronization

>A **cooperating process** is one that can affect or be affected by other processes executing in the system. 
>
>In this chapter, we discuss various mechanisms to ensure the orderly execution of cooperating processes that share a logical address space, so that data consistency is maintained.

### 5.1 Background

>We would arrive at this incorrect state because we allowed both processes to manipulate the variable counter concurrently. A situation like this, where several processes ***access and manipulate the same data concurrently*** and the outcome of the execution depends on the particular order in which the access takes place, is called a **race condition**. (竞态条件)
>
>To guard against the race condition above, we need to ensure that only one process at a time can be manipulating the variable. 
>
>To make such a guarantee, we require that the processes be synchronized in some way.

***

### 5.2 The Critical-Section Problem

> Consider a system consisting of n processes {P0, P1, ..., Pn−1}. Each process has a segment of code, called a ***critical section***, in which the process may be *changing common variables, updating a table, writing a file*, and so on. 
>
> The important feature of the system is that, when one process is executing in its critical section, no other process is allowed to execute in its critical section. 
>
> The critical-section problem is to design a protocol that the processes can use to cooperate. 
>
> Each process must ***request permission to enter*** its critical section. The section of code implementing this request is the **entry section**. 
>
> The critical section may be followed by an **exit section**. 
>
> The remaining code is the **remainder section**. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201118090417702.png" alt="image-20201118090417702" style="zoom:50%;" />

- A solution to the critical-section problem must satisfy the following three requirements:
  - **Mutual exclusion**. If process P~i~ is executing in its critical section, then no other processes can be executing in their critical sections.
  - **Progress**. If no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next, and this selection cannot be postponed indefinitely.
  - **Bounded waiting.** There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections.

- Two general approaches are used to handle critical sections in operating systems: 
  - **preemptive kernels** and **nonpreemptive kernels**. 
  - A preemptive kernel allows a process to be preempted(抢占) while it is running in kernel mode. 
  - A nonpreemptive kernel does not allow a process running in kernel mode to be preempted; a kernel-mode process will run until it exits kernel mode, blocks, or voluntarily yields control of the CPU.

***

### 5.3 Peterson’s Solution

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201118093558519.png" alt="image-20201118093558519" style="zoom:50%;" />

> Peterson’s solution is restricted to two processes that alternate execution between their critical sections and remainder sections. 
>
> The variable **turn** indicates whose turn it is to enter its critical section. That is, if turn == i, then process P~i~ is allowed to execute in its critical section. 
>
> The **flag** array is used to indicate if a process is ready to enter its critical section.
> For example, if flag[i] is true, this value indicates that P~i~ is ready to enter its critical section. 

***

### 5.4 Synchronization Hardware

>In the following discussions, we explore several more solutions to the critical-section problem using techniques ranging from hardware to software-based APIs available to both kernel developers and application programmers. 
>
>All these solutions are based on the premise of **locking** —that is, protecting critical regions through the use of **locks**. 

<img src=".\image-20201118100054570.png" alt="image-20201118100054570" style="zoom:50%;" />

<img src=".\image-20201118100109614.png" alt="image-20201118100109614" style="zoom:50%;" />

- The important characteristic of this instruction is that it is executed atomically.
  - Thus, if two ***test_and_set()*** instructions are executed simultaneously (each on a different CPU), they will be executed sequentially in some arbitrary order.

- If the machine supports the test_and_set() instruction, then we can implement mutual exclusion by declaring a boolean variable lock, initialized to false.

***

- The ***compare_and_swap()*** instruction, in contrast to the test_and_set() instruction, operates on three operands.

<img src=".\image-20201118101100034.png" alt="image-20201118101100034" style="zoom:50%;" /><img src=".\image-20201118101240271.png" alt="image-20201118101240271" style="zoom:50%;" />

>Mutual exclusion can be provided as follows: a global variable (lock) is declared and is initialized to 0. 
>
>The first process that invokes compare_and_swap() will set lock to 1. 
>
>Subsequent calls to compare_and_swap() will not succeed, because lock now is not equal to the expected value of 0. 
>
>When a process exits its critical section, it sets lock back to 0, which allows another process to enter its critical section.

****

### 5.5 Mutex Locks

> The simplest of these tools is the mutex lock. (In fact, the term mutex is short for mutual exclusion.)
>
> We use the mutex lock to protect critical regions and thus prevent race conditions. 
>
> That is, a process must acquire the lock before entering a critical section; it releases the lock when it exits the critical section.  
>
> The acquire()function acquires the lock, and the release() function releases the lock

```c
acquire() {
    while (!available)
    	; /* busy wait */
    available = false;;
}

do {
    acquire lock
    	critical section
    release lock
    	remainder section
} while (true);
```

- A mutex lock has a boolean variable available whose value indicates if the lock is available or not. 

  A process that attempts to acquire an unavailable lock is blocked until the lock is released.

>The main disadvantage of the implementation given here is that it requires **busy waiting**. 
>
>While a process is in its critical section, any other process that tries to enter its critical section must ***loop continuously*** in the call to acquire().
>
>This continual looping is clearly a problem in a real multiprogramming system, where a single CPU is shared among many processes. 
>
>Busy waiting ***wastes CPU cycles*** that some other process might be able to use productively.

***

### 5.6 Semaphores

>A **semaphore** S is an integer variable that, apart from initialization, is accessed only through two standard atomic operations: wait() and signal().

- The definition of wait() is as follows:

````c
wait(S) {
    while (S <= 0)
    ; // busy wait
    S--;
}
````

- The definition of signal() is as follows:

```c
signal(S) {
	S++;
}
```

- All modifications to the integer value of the semaphore in the wait() and signal() operations must be executed indivisibly. 

***

**Semaphore Usage**

>Operating systems often distinguish between counting and binary semaphores.
>
>The value of a **counting semaphore** can range over an unrestricted domain. 
>
>The value of a **binary semaphore** can range only between 0 and 1. 

- Counting semaphores can be used to control access to a given resource consisting of a finite number of instances. 
  - The semaphore is initialized to the number of resources available. 
  - Each process that wishes to use a resource performs a wait() operation on the semaphore (thereby decrementing the count). 
  - When the count for the semaphore goes to 0, all resources are being used. After that, processes that wish to use a resource will block until the count becomes greater than 0.

***

**Semaphore Implementation**

>To overcome the need for busy waiting, we can modify the definition of the wait() and signal() operations as follows: 
>
>- When a process executes the wait() operation and finds that the semaphore value is not positive, it must wait. 
>- However, rather than engaging in busy waiting, the process can block itself. 
>- The block operation places a process into a waiting queue associated with the semaphore, and the state of the process is switched to the waiting state. 
> - Then control is transferred to the CPU scheduler, which selects another process to execute.

**Deadlocks and Starvation**

>The implementation of a semaphore with a waiting queue may result in a situation where two or more processes are ***waiting indefinitely*** for an event that can be caused only by one of the waiting processes. 
>
>The event in question is the execution of a signal() operation. 
>
>When such a state is reached, these processes are said to be **deadlocked**.

***

### 5.7 Classic Problems of Synchronization

**The Dining-Philosophers Problem**

>Consider five philosophers who spend their lives thinking and eating. The philosophers share a circular table surrounded by five chairs, each belonging to one philosopher. In the center of the table is a bowl of rice, and the table is laid with five single chopsticks.
>
>When a philosopher thinks, she does not interact with her colleagues. 
>
>From time to time, a philosopher gets hungry and tries to pick up the two chopsticks that are closest to her.

<img src=".\image-20201119093836077.png" alt="image-20201119093836077" style="zoom:50%;" />

- It is a simple representation of the need to allocate several resources among several processes in a deadlock-free and starvation-free manner.

- One simple solution is to represent each chopstick with a semaphore. 

  - A philosopher tries to grab a chopstick by executing a **wait()** operation on that semaphore. 

  - She releases her chopsticks by executing the **signal()** operation on the appropriate semaphores. Thus, the shared data are 

    ```c
    semaphore chopstick[5];
    
    do {
        wait(chopstick[i]);
        wait(chopstick[(i+1) % 5]);
        . . .
        /* eat for awhile */
        . . .
        signal(chopstick[i]);
        signal(chopstick[(i+1) % 5]);
        . . .
        /* think for awhile */
        . . .
    } while (true);
    ```

    where all the elements of chopstick are initialized to 1.

- Several possible remedies to the deadlock problem are replaced by:

  - Allow at most four philosophers to be sitting simultaneously at the table.
  - Allow a philosopher to pick up her chopsticks only if both chopsticks are available (to do this, she must pick them up in a critical section). 
  - Use an asymmetric solution—that is, an odd-numbered philosopher picks up first her left chopstick and then her right chopstick, whereas an even-numbered philosopher picks up her right chopstick and then her left chopstick.

***

### 5.8 Monitors

>Examples illustrate that various types of errors can be generated easily when programmers use semaphores incorrectly to solve the critical-section problem. 

- In this section, we describe one fundamental high-level synchronization construct—the **monitor** type.

```c
monitor monitor name
{
/* shared variable declarations */
    function P1 ( . . . ) {
    	. . .
    }
    function P2 ( . . . ) {
    	. . .
    }
        .
        .
        .
    function Pn ( . . . ) {
    	. . .
    }
    initialization_code ( . . . ) {
    	. . .
    }
}
```

The syntax of a monitor type is shown above.

**Monitor Usage**

> A monitor type is an ADT that includes a set of ***programmer-defined operations*** that are provided with mutual exclusion within the monitor.
>
> The monitor type also declares the ***variables*** whose values define the ***state*** of an instance of that type, along with the ***bodies of functions*** that operate on those variables. 

- However, the monitor construct, as defined so far, is not sufficiently powerful for modeling some synchronization schemes. 

  - For this purpose, we need to define additional synchronization mechanisms. 

  - These mechanisms are provided by the **condition** construct. 

  - A programmer who needs to write a tailor-made synchronization scheme can define one or more variables of type condition: 

    `condition x, y;`

<img src=".\11.png" style="zoom:33%;" />

- The only operations that can be invoked on a condition variable are wait() and signal(). 
  - The operation
    `x.wait();`
    means that the process invoking this operation is suspended until another process invokes
    `x.signal();`

***

**Dining-Philosophers Solution Using Monitors**

> This solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are available. 
>
> To code this solution, we need to distinguish among three states in which we may
> find a philosopher. For this purpose, we introduce the following data structure:
> `enum {THINKING, HUNGRY, EATING} state[5];`
>
> Philosopher i can set the variable state[i] = EATING only if her two neighbors are not eating: `(state[(i+4) % 5] != EATING) and (state[(i+1) % 5] != EATING).`

```c
monitor DiningPhilosophers
{
    enum {THINKING, HUNGRY, EATING} state[5];
    condition self[5];
    void pickup(int i) {
        state[i] = HUNGRY;
        test(i);
        if (state[i] != EATING)
        	self[i].wait();
    }
    void putdown(int i) {
        state[i] = THINKING;
        test((i + 4) % 5);
        test((i + 1) % 5);
    }
    void test(int i) {
        if ((state[(i + 4) % 5] != EATING) &&
        	(state[i] == HUNGRY) &&
        	(state[(i + 1) % 5] != EATING)) {
            state[i] = EATING;
            self[i].signal();
    	}	
    }
    initialization_code() {
        for (int i = 0; i < 5; i++)
        	state[i] = THINKING;
    }
}
```

- We also need to declare
  `condition self[5];`
  This allows philosopher i to delay herself when she is hungry but is unable to obtain the chopsticks she needs.

***

**Resuming Processes within a Monitor**

>How do we determine which of the suspended processes should be resumed next? One simple solution is to use a ***first-come, first-served (FCFS)*** ordering, so that the process that has been waiting the longest is resumed first. 
>
>In many circumstances, however, such a simple scheduling scheme is not adequate. 
>
>For this purpose, the **conditional-wait** construct can be used. This construct has the form
>
>`x.wait(c);`
>
>where c is an integer expression that is evaluated when the wait() operation is executed. The value of c, which is called a **priority number**, is then stored with the name of the process that is suspended. 
>
>When x.signal() is executed, the process with the smallest priority number is resumed next.

****

### 5.9 Synchronization Examples

**Synchronization in Windows**

***

## 6. CPU scheduling

### 6.1 Basic Concepts

**CPU–I/O Burst Cycle**

>The success of CPU scheduling depends on an observed property of processes: process execution consists of a **cycle** of CPU execution and I/O wait. 

- Processes alternate between these two states. Process execution begins with a CPU burst. That is followed by an I/O burst, which is followed by another CPU burst, then another I/O burst, and so on. 

<img src=".\12.png" style="zoom: 33%;" />

- The durations of CPU bursts have been measured extensively. 

<img src=".\13.png" style="zoom:33%;" />

***

**CPU Scheduler**

>Whenever the CPU becomes idle, the operating system must select one of the processes in the ready queue to be executed. The selection process is carried out by the **short-term scheduler**, or CPU scheduler. 

**Preemptive Scheduling**

- CPU-scheduling decisions may take place under the following four circumstances:
  - When a process switches from the running state to the waiting state.
  - When a process switches from the running state to the ready state (for example, when an interrupt occurs)
  - When a process switches from the waiting state to the ready state (for example, at completion of I/O)
  - When a process terminates

>When scheduling takes place only under circumstances 1 and 4, we say that the scheduling scheme is **nonpreemptive** or **cooperative**. Otherwise, it is **preemptive**. 

***

### 6.2 Scheduling Criteria





***

### 6.3 Scheduling Algorithms

**First-Come, First-Served Scheduling**

> By far the simplest CPU-scheduling algorithm is the first-come, first-served (**FCFS**) scheduling algorithm. 
>
> With this scheme, the process that requests the CPU first is allocated the CPU first. 

- On the negative side, the average waiting time under the FCFS policy is often quite long. 

**Shortest-Job-First Scheduling**

>A differentapproachto CPU schedulingisthe shortest-job-first (SJF)scheduling algorithm. This algorithm associates with each process the length of the process’s next CPU burst. 

- With short-term scheduling, there is no way to know the length of the next CPU burst. 
  - One approach to this problem is to try to approximate SJF scheduling. 
  - We may not know the length of the next CPU burst, but we may be able to predict its value. We expect that the next CPU burst will be similar in length to the previous ones. 
  - The next CPU burst is generally predicted as an exponential average of the measured lengths of previous CPU bursts. 
  - We can define the exponential average with the following formula. 

$$
for \;\alpha:\\
\tau_{n+1} = \alpha t_n+(1-\alpha)\tau_n
$$

- The value of $t_n$ contains our most recent information, while $\tau_n$ stores the past history. 

***

**Priority Scheduling**

> The SJF algorithm is a special case of the general **priority-scheduling algorithm.**

>A priority is associated with each process, and the CPU is allocated to the process with the highest priority. 
>
>Equal-priority processes are scheduled in FCFS order.
>
>In this text, we assume that low numbers represent high priority.

***

### 6.4 Thread Scheduling

**Contention Scope**

lightweight process (LWP)

> One distinction between user-level and kernel-level threads lies in how they are scheduled. 
>
> On systems implementing the many-to-one and many-to-many models, the thread library schedules user-level threads to run on an available LWP. 
>
> This scheme is known as **process-contention scope (PCS)**, since competition for the CPU takes place among threads belonging to the same process. 
>
> To decide which kernel-level thread to schedule onto a CPU, the kernel uses **system-contention scope (SCS)**.
>
> Competition for the CPU with SCS scheduling takes place among all threads in the system. 
>
> Systems using the one-to-one model, such as Windows, Linux, and Solaris, schedule threads using only SCS.

***

**Pthread Scheduling**

>Pthreads identifies the following contention scope values:
>
>- `PTHREAD_SCOPE_PROCESS schedules threads using PCS scheduling.`
>- `PTHREAD_SCOPE_SYSTEM schedules threads using SCS scheduling.`
>
>The Pthread IPC provides two functions for getting—and setting—the contention scope policy:
>
>- `pthread_attr_setscope(pthread_attr_t *attr, int scope)`
>- `pthread_attr_getscope(pthread_attr_t *attr, int *scope)`

***

### 6.5 Multiple-Processor Scheduling

**Approaches to Multiple-Processor Scheduling**

>One approach to CPU scheduling in a multiprocessor system has all scheduling decisions, I/O processing, and other system activities handled by a single processor—the master server. The other processors execute only user code. This **asymmetric multiprocessing** is simple because only one processor accesses the system data structures, reducing the need for data sharing.
>
>A second approach uses **symmetric multiprocessing (SMP)**, where each processor is self-scheduling. All processes may be in a common ready queue, or each processor may have its own private queue of ready processes. 

**Processor Affinity**

> When a process has been running on a specific processor. The data most recently accessed by the process populate the cache for the processor. 
>
> If the process migrates to another processor. The contents of cache memory must be invalidated for the first processor, and the cache for the second processor must be repopulated. 
>
> Because of the high cost of invalidating and repopulating caches, most SMP systems try to avoid migration of processes from one processor to another and instead attempt to keep a process running
> on the same processor. 
>
> This is known as **processor affinity**—that is, a process has an affinity for the processor on which it is currently running.

- When an operating system has a policy of attempting to keep a process running on the same processor—but not guaranteeing that it will do so—we have a situation known as **soft affinity.**
- In contrast, some systems provide system calls that support **hard affinity**, thereby allowing a process to specify a subset of processors on which it may run. 

**non-uniform memory access (NUMA)**, in which a CPU has faster access to some parts of main memory
than to other parts.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201125151923624.png" alt="image-20201125151923624" style="zoom:50%;" />

***

**Load Balancing**

>Load balancing attempts to keep the workload evenly distributed across all processors in an SMP system. 

- There are two general approaches to load balancing: **push migration** and **pull migration**. 
  - With push migration, a specific task periodically checks the load on each processor and—if it finds an imbalance—evenly distributes the load by moving (or pushing) processes from overloaded to idle or less-busy processors. 
  - Pull migration occurs when an idle processor pulls a waiting task from a busy processor. 

***

### 6.6 Real-Time CPU Scheduling

> In general, we can distinguish between soft real-time systems and hard real-time systems. 
>
> **Soft real-time systems** provide no guarantee as to when a critical real-time process will be scheduled. They guarantee only that the process will be given preference over noncritical processes. 
>
> **Hard real-time systems** have stricter requirements. A task must be serviced by its deadline; service after the deadline has expired is the same as no service at all. 

**Minimizing Latency**

>When an event occurs, the system must respond to and service it as quickly as possible. 
>
>We refer to **event latency** as the amount of time that elapses from when an event occurs to when it is serviced.

- Two types of latencies affect the performance of real-time systems:

1. Interrupt latency
2. Dispatch latency

- **Interrupt latency** refers to the period of time from the arrival of an interrupt at the CPU to the start of the routine that services the interrupt. 
- 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201126211732277.png" alt="image-20201126211732277" style="zoom: 50%;" />

***

- The amount of time required for the scheduling dispatcher to stop one process and start another is known as **dispatch latency**. 

- The most effective technique for keeping dispatch latency low is to provide preemptive kernels.

- The conflict phase of dispatch latency has two components:
  - Preemption of any process running in the kernel
  - Release by low-priority processes of resources needed by a high-priority process

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\Q5RNTIV$K1DL5T1[L8LVQ.png" alt="img" style="zoom:50%;" />

***

## 7. Memory Management

### 7.1 No memory abstraction

<img src=".\image-20201201195948278.png" alt="image-20201201195948278" style="zoom:50%;" />

> Absolutely physical address can cause many drawbacks.
>
> First, it’s hard to run multiple progress since each user progress may interfering each other by using physical address like the figure below. The JMP instruction has an error address because the progress has no private address space.
>
> An approach to solve these problems is to add the last address number to every instructions in the second progress. After all, the ‘‘28’’  Fig. 3-2(b) has to be relocated but an instruction like
> `MOV REGISTER1,28`
> which moves the number 28 to REGISTER1 must not be relocated. The loader needs some way to tell what is an **address** and what is a **constant**. 

<img src=".\c.png" style="zoom:50%;" />

***

### 7.2 A Memory Abstraction: Address Space

<b>**The Notion of an Address Space**</b>

>Two problems have to be solved to allow multiple applications to be in memory at the same time without interfering with each other: **protection** and **relocation**.

- An **address space** is the set of addresses that a process can use to address memory. 
- Each process has its own address space, independent of those belonging to other processes

**Swapping**

> The operation of a swapping system is illustrated in Fig. 3-4. 

![image-20201201203141533](.\image-20201201203141533.png)

>When swapping creates multiple holes in memory, it is possible to combine them all into one big one by moving all the processes downward as far as possible. 
>
>This technique is known as **memory compaction**. 

- If it is expected that most processes will grow as they run, it is probably a good idea to allocate a little extra memory whenever a process is swapped in or moved, to reduce the overhead associated with moving or swapping processes that no longer fit in their allocated memory. 

***

### 7.3 Virtual Memory

>The basic idea behind virtual memory is that each program has its ***own address space***, which is broken up into chunks called **pages**. 
>
>Each page is a ***contiguous range of addresses***. These pages are mapped onto physical memory, but not all pages have to be in physical memory at the same time to run the program. 

- When the program references a part of its address space that is in physical memory, the hardware performs the necessary **mapping** on the fly. 
- When the program references a part of its address space that is not in physical memory, the operating system is alerted to go ***get the missing piece*** and re-execute the instruction that failed.

**Paging**

>Addresses can be generated using indexing, base registers, segment registers, and other ways.
>
>These program-generated addresses are called **virtual addresses** and form the **virtual address space**. 
>
>When virtual memory is used, the virtual addresses do not go directly to the ***memory bus***. Instead, they go to an ***MMU (Memory Management Unit)*** that maps the virtual addresses onto the physical memory addresses.

<img src=".\image-20201203141808403.png" alt="image-20201203141808403" style="zoom:50%;" />

***

>When the program tries to access address 0, for example, using the instruction
>`MOV REG,0`
>virtual address 0 is sent to the MMU. The MMU sees that this virtual address falls in page 0 (0 to 4095), which according to its mapping is page frame 2 (8192 to 12287). It thus transforms the address to 8192 and outputs address 8192 onto the bus. 

<img src=".\image-20201203142139277.png" alt="image-20201203142139277" style="zoom:50%;" />

>The virtual address space consists of fixed-size units called pages. 
>
>The corresponding units in the physical memory are called **page frames**. 
>
>The pages and page frames are generally the same size. The page size ranges from 4KB to 1GB.
>
>We could use 4-KB pages for user applications and a single 1-GB page for the kernel. 

- In the actual hardware, a **Present/absent bit** keeps track of which pages are physically present in memory.

>What happens if the program references an unmapped address, for example, by using the instruction
>`MOV REG,32780`
>which is byte 12 within virtual page 8? 

- The MMU notices that the page is ***unmapped*** (indicated by a cross in the figure) and causes the CPU to **trap** to the operating system. This trap is called a **page fault**. 
- The operating system picks a ***little-used*** page frame and writes its contents back to the disk 

- It then ***fetches*** (also from the disk) the page that was just referenced into the page frame just freed, changes the map, and ***restarts*** the trapped instruction.

***

>Now let us look inside the MMU to see how it works and why we hav e chosen to use a page size that is a power of 2. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\d.png" style="zoom:50%;" />

- The incoming 16-bit virtual address is split into a 4-bit page number and a 12-bit offset. 
  - With 4 bits for the page number, we can have 16 pages
  - and with 12 bits for the offset, we can address all 4096 bytes within a page.
- The page number is used as an index into the **page table**, yielding the number of the page frame corresponding to that virtual page. 
- If the bit is 1, the page frame number found in the page table is copied to the ***high-order 3 bits*** of the output register, along with the 12-bit offset.

- The output register is then put onto the memory bus as the physical memory address.

***

**Page Tables**

>Thus, the purpose of the page table is to map virtual pages onto page frames.
>Mathematically speaking, the page table is a function, with the virtual page number as argument and the physical frame number as result. 

**Structure of a Page Table Entry**

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201203150957571.png" alt="image-20201203150957571" style="zoom:50%;" />

- The most important field is the ***Page frame number***. 
- Next to it we have the ***Present/absent bit***.
- The ***Protection bits*** tell what kinds of access are permitted. 
  - In the simplest form, this field contains 1 bit, with 0 for read/write and 1 for read only. 
  - A more sophisticated arrangement is having 3 bits, one bit each for enabling reading, writing, and executing the page.
- The ***Modified and Referenced bits*** keep track of page usage. 
- When a page is ***written*** to, the hardware automatically sets the **Modified bit**. 
  - This bit is of value when the operating system decides to reclaim a page frame. 
  - If the page in it has been modified (i.e., is ‘‘dirty’’), it must be written back to the disk.
  - The bit is sometimes called the ***dirty bit***, since it reflects the page’s state.

- ***The Referenced bit*** is set whenever a page is referenced, either for reading or for writing. Its value is used to ***help the operating system choose a page to evict*** when a page fault occurs.
- Finally, the last bit allows caching to be disabled for the page. This feature is important for pages that map onto device registers rather than memory. 

***

**Speeding Up Paging**

>In any paging system, two major issues must be faced:
>
>- The mapping from virtual address to physical address must be fast.
>- If the virtual address space is large, the page table will be large.
**Translation Lookaside Buffers**

>TLBs can be used to speed up virtual-to-physical address translation over the original page-table-in-memory scheme. 

It is possible that the program simply accessed an invalid address and no mapping needs to be added in the TLB at all. 

In that case, the operating system typically kills the program with a **segmentation fault**. 

***

**Page Tables for Large Memories**

> As a first approach, consider the use of a **multilevel page table**.

- The secret to the multilevel page table method is to ***avoid keeping all the page tables in memory*** all the time. In particular, those that are not needed should not be kept around. 
- We have a 32-bit virtual address that is partitioned into a 10-bit PT1 field, a 10-bit PT2 field, and a 12-bit Offset field. 
- Since offsets are 12 bits, pages are 4 KB, and there are a total of 2^20^ of them.

<img src=".\e.png" style="zoom:50%;" />

***

>An alternative to ever-increasing levels in a paging hierarchy is known as **inverted page tables**.

- In this design, there is ***one entry per page frame*** in real memory, rather than one entry per page of virtual address space.

***

### 7.4 Page Replacement Algorithms

**The Not Recently Used Page Replacement Algorithm**

****

## 8. File System

### 8.1 Files

**File Naming**

> A file is an abstraction mechanism. It provides a way to store information on the disk and read it back later.  

**File Types**

> **Regular files** are the ones that contain user information. 
>
> **Character special files** are related to input/output and used to model serial I/O devices, such as terminals, printers, and networks. 
>
> **Block special files** are used to model disks. 

```c
/* File copy program. Error checking and reporting is minimal. */
#include <sys/types.h>
/* include necessary header files */
#include <fcntl.h>
#include <stdlib.h>
#include <unistd.h>
int main(int argc, char *argv[]);
/* ANSI prototype */
#define BUF_SIZE 4096
/* use a buffer size of 4096 bytes */
#define OUTPUT_MODE 0700
/* protection bits for output file */
int main(int argc, char *argv[])
{
    int in_fd, out_fd, rd_count, wt_count;
    char buffer[BUF_SIZE];
    if (argc != 3) 
        exit(1);	/* syntax error if argc is not 3 */
    /* Open the input file and create the output file */
    in_fd = open(argv[1], O_RDONLY); /* open the source file */
    if (in_fd < 0) 
        exit(2);		/* if it cannot be opened, exit */
    out_fd = creat(argv[2], OUTPUT_MODE); 
    /* create the destination file */
    if (out_fd < 0) 
        exit(3);
    /* if it cannot be created, exit */
    /* Copy loop */
    while (TRUE) {
        rd_count = read(in_fd, buffer, BUF_SIZE); /* read a block of data */
        if (rd_count <= 0)  /* if end of file or error, exit loop */
            break;
        wt_count = write(out_fd, buffer, rd_count); /* write data */
        if (wt_count <= 0)  /* wt count <= 0 is an error */
            exit(4);    
    }
    /* Close the files */
    close(in_fd);
    close(out_fd);
    if (rd_count == 0)	/* no error on last read */
    	exit(0);
    else
    	exit(5);  /* error on last read */
}
```

***

### 8.3 File-system Implementation

**File-System Layout**

>File systems are stored on disks. 
>
>Most disks can be divided up into one or more partitions, with independent file systems on each partition. 
>
>Sector 0 of the disk is called the **MBR (Master Boot Record)** and is used to boot the computer.
>
>The end of the MBR contains the **partition table**. This table gives the starting and ending addresses of each partition. 
>
>One of the partitions in the table is marked as active. 

- When the computer is booted, the BIOS reads in and executes the MBR.
- The first thing the MBR program does is ***locate the active partition***, read in its first block, which is called the boot block, and execute it. 
- The program in the boot block loads the operating system contained in that partition. 

<img src=".\image-20201210152338284.png" alt="image-20201210152338284" style="zoom:50%;" />

- The first one is the **superblock**. It contains all the key parameters about the file system and is read into memory when the computer is booted or the file system is first touched. Typical information in the superblock includes a magic number to identify the file-system type, the number of blocks in the file system, and other key administrative information.

***

- #### **Implementing Files**

**Contiguous Allocation**

> The simplest allocation scheme is to store each file as a contiguous run of disk blocks. 

**Linked-List Allocation**

>The second method for storing files is to keep each one as a linked list of disk blocks, as shown in Fig. 4-11. 

<img src=".\image-20201210195614520.png" alt="image-20201210195614520" style="zoom:50%;" />

>Both disadvantages of the linked-list allocation can be eliminated by taking the pointer word from each disk block and **putting it in a *table* in memory**. 

<img src=".\M3YZ9@R77VG0AHTK45L[_{2.png" alt="img" style="zoom:50%;" />

- File A uses disk blocks 4, 7, 2, 10, and 12, in that order, and file B uses disk blocks 6, 3, 11, and 14, in that order. 
- Both chains are terminated with a special marker (e.g., −1) that is not a valid block number. 
- Such a table in main memory is called a **FAT** (**File Allocation Table**).

- The primary disadvantage of this method is that the entire table must be in memory all the time to make it work. 

Clearly the FAT idea does not scale well to large disks. 

***

**I-nodes**

>Our last method for keeping track of which blocks belong to which file is to associate with each file a data structure called an **i-node (index-node)**, which lists the attributes and disk addresses of the file’s blocks. 

<img src=".\image-20201210212055734.png" alt="image-20201210212055734" style="zoom:50%;" />

- The big advantage of this scheme over linked files using an in-memory table is that the i-node need be in memory only when the corresponding file is open. 

***

**Implementing Directories**

>Before a file can be read, it must be opened. When a file is opened, the operating system uses the path name supplied by the user to locate the directory entry on the disk. 
>
>The main function of the directory system is to ***map the ASCII name*** of the file onto the ***information*** needed to locate the data.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201210212839072.png" alt="image-20201210212839072" style="zoom:50%;" />

- A closely related issue is where the attributes should be stored. 
  - One obvious possibility is to store them directly in the directory entry. 

- One alternative is to give up the idea that all directory entries are the same size.
  - With this method, each directory entry contains a fixed portion, typically starting with the length of the entry.

<img src=".\image-20201210214124123.png" alt="image-20201210214124123" style="zoom:50%;" />

- A disadvantage of this method is that when a file is removed, a ***variable-sized gap*** is introduced into the directory into which the next file to be entered may not fit. 

- Another way to handle variable-length names is to make the directory entries themselves all fixed length and ***keep the file names together in a heap*** at the end of the directory, as shown in Fig. 4-15(b). 

***

**Shared Files**

>Figure 4-16 shows the file system with one of C’s files now present in one of B’s directories as well. 

<img src=".\image-20201210215417468.png" alt="image-20201210215417468" style="zoom:50%;" />

- The connection between B’s directory and the shared file is called a **link**
- The file system itself is now a **Directed Acyclic Graph**, or DAG

***

**Log-Structured File Systems**

> The basic idea is to structure the entire disk as a great big log.

- Periodically, and when there is a special need for it, ***all the pending writes being buffered*** in memory are ***collected into a single segment*** and written to the disk as a single contiguous segment at the end of the log. 
  - A single segment may thus contain i-nodes, directory blocks, and data blocks, all mixed together. 
  - At the start of each segment is a segment summary, telling what can be found in the segment. 
  - If the average segment can be made to be about 1 MB, almost the full bandwidth of the disk can be utilized.

- If a file is overwritten, its i-node will now point to the new blocks, but the old ones will still be

  occupying space in previously written segments.

  - To deal with this problem, LFS has a cleaner thread that spends its time scanning the log circularly to compact it. 

***

**Journaling File Systems**

>The basic idea here is to keep a log of what the file system is going to do before it does it, so that if the system crashes before it can do its planned work, upon rebooting the system can look in the log to see what was going on at the time of the crash and finish the job. 

- To make journaling work, the logged operations must be **idempotent**, which means they can be repeated as often as necessary without harm. 

***

**Virtual File Systems**

***

### 4.4 File-system management and opitimize

> #### Disk-Space Management

**Block size**

**Keeping Track of Free Blocks**

**Disk Quotas**

> To prevent people from hogging too much disk space.

<img src=".\image-20201214144045378.png" alt="image-20201214144045378" style="zoom:50%;" />

- When a user opens a file, the attributes and disk addresses are located and put into an open-file table in main memory. Among the attributes is an entry telling who the owner is. Any increases in the file’s size will be charged to the owner’s quota.

- A second table contains the quota record for every user with a currently open file, even if the file was opened by someone else. 

**File-System Backups**

>A **logical dump** starts at one or more specified directories and recursively dumps all files and directories found there that have changed since some given base date. 

<img src=".\A$ZUVI}CSY6$97NR}QEV{6.png" alt="img" style="zoom:50%;" />

- In the figure we see a file tree with directories (squares) and files (circles).
- 

****

## 13. I/O system

### 13.2 I/O hardware

> The device communicates with the machine via a connection point, or **port**.
>
> A **bus** is a set of wires and a rigidly defined protocol that specifies a set of messages that can be sent on the wires. 

<img src=".\V_R%I7`H4N@FVLI9KNHKX2.png" alt="img" style="zoom:50%;" />

> In the figure, a **PCI** bus (the common PC system bus) connects the processor–memory subsystem to fast devices.
>
> An expansion bus connects relatively slow devices, such as the keyboard and serial and USB ports. 
>
> In the upper-right portion of the figure, four disks are connected together on a **Small Computer System Interface** (SCSI) bus plugged into a SCSI controller. 
>
> A **controller** is a collection of electronics that can operate a port, a bus, or a device. A serial-port controller is a simple device controller. It is a single chip (or portion of a chip) in the computer that controls the signals on the wires of a serial port. 

- Controller has one or more ***registers*** for data and control signals. 
- The processor communicates with the controller by reading and writing bit patterns in these registers. 

- An I/O port typically consists of four registers, called the **status**, **control**, **data-in**, and **data-out** registers.
  - The data-in register is read by the host to get input.
  - The data-out register is written by the host to send output.
  - The status register contains bits that can be read by the host. These bits indicate states.
  - The control register can be written by the host to start a command or to change the mode of a device. 

<img src=".\JN$T9`4@P0IV}Z_JJ3_GC5X.png" alt="img" style="zoom:50%;" />

***

**A Canonical Device**

<img src=".\image-20201217102110048.png" alt="image-20201217102110048" style="zoom:50%;" />

- A device has two important components. 
  - The first is the hardware **interface** it presents to the rest of the system. 
  - The second part of any device is its **internal structure**.

**The Canonical Protocol**

- In the picture above, the (simplified) device interface is comprised of three registers: 
  - a **status** register, which can be read to see the current status of the device; 
  - a **command** register, to tell the device to perform a certain task; 
  - a **data** register to pass data to the device, or get data from the device. 

- The protocol is as follows:

```c
While (STATUS == BUSY)
	; // wait until device is not busy
Write data to DATA register
Write command to COMMAND register
	(starts the device and executes the command)
While (STATUS == BUSY)
	; // wait until device is done with your request
```

- The protocol has four steps. 
  - In the first, the OS ***waits*** until the device is ready to receive a command by ***repeatedly reading the status register***; 
    - we call this **polling** the device (basically, just asking it what is going on)
  - Second, the OS ***sends*** some data down to the data register;
    - When the main CPU is involved with the data movement (as in this example protocol), we refer to it as **programmed I/O** (**PIO**). 
  - Third, the OS ***writes*** a command to the command register; 
    - Doing so implicitly lets the device know that both the data is present and that it should begin working on the command.
  - Finally, the OS waits for the device to finish by again polling it in a loop, waiting to see if it is finished

***

Ruemmler and Wilkes [RW92] and Anderson, Dykes, and Riedel [ADR03]

### 13.3 Hard Disk Drives

**Basic Geometry**

- We start with a **platter**
  - A disk may have one or more platters; each platter has 2 sides, each of which is called a **surface**. 
  - The platters are all bound together around the spindle, which is connected to a **motor** that spins the platters around。
  - The rate of rotation is often measured in **rotations per minute** (**RPM**), and typical modern values are in the 7,200 RPM to 15,000 RPM range. 
  - Data is encoded on each surface in concentric circles of sectors; we call one such concentric circle a **track**.
  - This process of reading and writing is accomplished by the **disk head**; 
    - there is one such head per surface of the drive. 
    - The disk head is attached to a single **disk arm**, which moves across the surface to position the head over the desired track.

<img src=".\image-20201217150604470.png" alt="image-20201217150604470" style="zoom:50%;" />

***

**A Simple Disk Drive**

> Single-track Latency: The Rotational Delay

- How a request would be processed on our simple, onetrack disk
  - The disk head must just wait for the desired sector to rotate under the disk head.
  -  It has a special name: **rotational delay** (sometimes rotation delay)

> Multiple Tracks: Seek Time

- To understand how the drive might access a given sector, we now trace what would happen on a request to a distant sector, a read to sector 11. 
  - To service this read, the drive has to first ***move the disk arm to the correct track*** (in this case, the outermost one), in a process known as a **seek**.

<img src=".\image-20201217151848722.png" alt="image-20201217151848722" style="zoom:50%;" />

And thus, we have a complete picture of I/O time: first a seek, then waiting for the rotational delay, and finally the transfer.

***

### 13.4 Redundant Arrays of Inexpensive Disks (RAIDs)

**How To Evaluate A RAID**

- Specifically, we will evaluate each RAID design along three axes. 
  - The first axis is **capacity**; 
    - given a set of N disks each with B blocks, how much useful capacity is available to clients of the RAID?
  - The second axis of evaluation is **reliability**. 
    - How many disk faults can the given design tolerate? 
  - Finally, the third axis is **performance**. 
    - Performance is somewhat challenging to evaluate, because it depends heavily on the workload presented to the disk array.

***

**RAID Level 0: Striping**

> The simplest form of striping will stripe blocks across the disks of the system as follows (assume here a 4-disk array):

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201217214940891.png" alt="image-20201217214940891" style="zoom:50%;" />

- The basic idea: spread the blocks of the array across the disks in a round-robin fashion. 
  - This approach is designed to ***extract the most parallelism from the array*** when requests are made for contiguous chunks of the array.
-  We call the blocks in the same row a stripe; thus, blocks 0, 1, 2, and 3 are in the same stripe above.
- For example, we could arrange the blocks across disks as in Figure 38.2:

<img src=".\image-20201218093225336.png" alt="image-20201218093225336" style="zoom:50%;" />

- In this example, we place two 4KB blocks on each disk before moving on to the next disk. 
  - Thus, the **chunk size** of this RAID array is 8KB, and a stripe thus consists of 4 chunks or 32KB of data.

**Chunk Sizes**

> Chunk size mostly affects ***performance*** of the array. 
>
> For example, a small chunk size implies that many files will ***get striped across many disks***, thus increasing the parallelism of reads and writes to a single file;

****

 **RAID Level 1: Mirroring**

> With a mirrored system, we simply make more than one copy of each block in the system; 
>
> each copy should be placed on a separate disk, of course. By doing so, we can ***tolerate disk failures***.

- In a typical mirrored system, we will assume that for each logical block, the RAID keeps two physical copies of it.

<img src=".\image-20201218095327805.png" alt="image-20201218095327805" style="zoom:50%;" />

***

**RAID Level 4: Saving Space With Parity**

> Parity-based approaches attempt to use less capacity and thus overcome the huge space penalty paid by mirrored systems.
>
> They do so at a cost, however: performance.

<img src=".\image-20201218101651768.png" alt="image-20201218101651768" style="zoom:50%;" />

- For each stripe of data, we have added a single parity block that stores the redundant information for that stripe of blocks.
- 