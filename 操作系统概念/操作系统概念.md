# 操作系统概念

## 2.Operating-System Structures

> An operating system provides the environment within which programs are executed. 

- To describe the services an operating system provides to users, processes, and other systems.
- To discuss the various ways of structuring an operating system.
- To explain how operating systems are installed and customized and how they boot.

***

### 2.1 Operating-System Services

>An operating system provides an environment for the execution of programs.
>It provides certain services to programs and to the users of those programs.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201106220804177.png" alt="image-20201106220804177" style="zoom:50%;" />

> One set of operating system services provides functions that are helpful to the user.
>
> **User interface.** Almost all operating systems have a user interface (UI). This interface can take several forms. 
>
> - One is a **command-line interface** (CLI), which uses text commands and a method for entering them (say, a keyboard for typing in commands in a specific format with specific options).
> - Another is a **batch interface**, in which commands and directives to control those commands are entered into files, and those files are executed.
> - Most commonly, a **graphical user interface (GUI)** is used.
>
> **Program execution.**
>
> - The system must be able to load a program into memory and to run that program. The program must be able to end its execution, either normally or abnormally.
>
> **I/O operations.** 
>
> - A running program may require I/O, which may involve a file or an I/O device. 
>
> **File-system manipulation.** 
>
> - The file system is of particular interest. Obviously, programs need to read and write files and directories.
>
> **Communications.** 
>
> - There are many circumstances in which one process needs to exchange information with another process. 
>   - Communications may be implemented via **shared memory**, in which two or more processes read and write to a shared section of memory,
>   - or **message passing**, in which packets of information in predefined formats are moved between processes by the operating system.
>
> **Error detection.** 
>
> - The operating system needs to be detecting and correcting errors constantly. 
>
> ***
>
> Another set of operating system functions exists not for helping the user but rather for ensuring the efficient operation of the system itself. 
>
> **Resource allocation.**
>
> - When there are multiple users or multiple jobs running at the same time, resources must be allocated to each of them. 
>
> **Accounting.** 
>
> - We want to keep track of which users use how much and what kinds of computer resources. 
>
> **Protection and security.** 
>
> - The owners of information stored in a multiuser or networked computer system may want to control use of that information.

***

### 2.2 User and Operating-System Interface

**Command Interpreters**

> Some operating systems include the command interpreter in the kernel.
>
> Others, such as Windows and UNIX, treat the command interpreter as a special program that is running when a job is initiated or when a user first logs on.

On systems with multiple command interpreters to choose from, the interpreters are known as **shells**. 

- on UNIX and Linux systems, a user may choose among several different shells, including the *Bourne shell, C shell, Bourne-Again shell, Korn shell,* and others. 

- The main function of the command interpreter is to get and execute the next user-specified command. 
  - Many of the commands given at this level manipulate files: *create, delete, list, print, copy, execute, and so on*. These commands can be implemented in two general ways.
    - In one approach, the command interpreter itself contains the code to execute the command. For example, a command to delete a file may cause the command interpreter to *jump to a section of its code that sets up the parameters and makes the appropriate system call.* 
    - An alternative approach—used by UNIX, among other operating systems—implements most commands through system programs. In this case, the command interpreter merely uses the command to *identify a file to be loaded into memory and executed*.

***

**Graphical User Interfaces**

> Users employ a mouse-based window- and-menu system characterized by a desktop metaphor. 
>
> The user moves the mouse to position its pointer on images, or icons, on the screen (the desktop)
> that represent programs, files, directories, and system functions.
>
> Depending on the mouse pointer’s location, clicking a button on the mouse can invoke a program, select a file or directory—known as a folder—or pull down a menu that contains commands.

***

**Choice of Interface**

> if a frequent task requires a set of command-line steps, those steps can be recorded into a file, and that file can be run just like a program. The program is not compiled into executable code but rather is interpreted by the command-line interface. These shell scripts are very common on systems that are command-line oriented.

***

### 2.3 System Calls

>System calls provide an interface to the services made available by an operating system. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201107100459021.png" alt="image-20201107100459021" style="zoom:50%;" />

>A programmer accesses an API via a library of code provided by the operating system. In the case of UNIX and Linux for programs written in the C language, the library is called **libc**.

***

### 2.4 Types of System Calls

>System calls can be grouped roughly into six major categories: **process control**, **file manipulation**, **device manipulation**, **information maintenance**, **communications**, and **protection**.

**Process Control**

>A running program needs to be able to halt its execution either normally (end()) or abnormally (abort()). 

More severe errors can be indicated by a higher-level error parameter. 

- **Process control**
  ◦ end, abort
  ◦ load, execute
  ◦ create process, terminate process
  ◦ get process attributes, set process attributes
  ◦ wait for time
  ◦ wait event, signal event
  ◦ allocate and free memory
- **File management**
  ◦ create file, delete file
  ◦ open, close
  ◦ read, write, reposition
  ◦ get file attributes, set file attributes
- **Device management**
  ◦ request device, release device
  ◦ read, write, reposition
  ◦ get device attributes, set device attributes
  ◦ logically attach or detach devices
- **Information maintenance**
  ◦ get time or date, set time or date
  ◦ get system data, set system data
  ◦ get process, file, or device attributes
  ◦ set process, file, or device attributes
- **Communications**
  ◦ create, delete communication connection
  ◦ send, receive messages
  ◦ transfer status information
  ◦ attach or detach remote devices

>Quite often, two or more processes may share data. To ensure the integrity of the data being shared, operating systems often provide system calls allowing a process to **lock** shared data. 
>
>Then, no other process can access the data until the lock is released. 

***

**File Management**

>We first need to be able to **create()** and **delete()** files. Either system call requires the name of the file and perhaps some of the file’s attributes. Once the file is created, we need to **open()** it and to use it. We may also **read()**, **write()**, or **reposition()** (rewind or skip to the end of the file, for example). Finally, we need to close() the file, indicating that we are no longer using it.

***

**Device Management**

>A process may need several resources to execute—main memory, disk drives, access to files, and so on. If the resources are available, they can be granted, and control can be returned to the user process. Otherwise, the process will have to wait until sufficient resources are available.
>
>The various resources controlled by the operating system can be thought of as devices. 
>
>A system with multiple users may require us to first **request()** a device, to ensure exclusive use of it. After we are finished with the device, we **release()** it. 
>
>Once the device has been requested (and allocated to us), we can read(), write(), and (possibly) reposition() the device, just as we can with files. 

***

**Information Maintenance**

>Many systems provide system calls to **dump()** memory. This provision is useful for debugging. A program **trace** lists each system call as it is executed. Even microprocessors provide a CPU mode known as **single step**, in which a trap is executed by the CPU after every instruction. The trap is usually caught by a debugger.

***

**Communication**

>There are two common models of interprocess communication: the **message-passing model** and the **shared-memory model**. 
>
>- In the message-passing model
>  - the communicating processes exchange messages with one another to transfer information. Messages can be exchanged between the processes either directly or indirectly through a common mailbox.
>  - Before communication can take place, a connection must be opened. The name of the other communicator must be known, be it another process on the same system or a process on another computer connected by a communications network. 
>  - Each process has a **process name**, and this name is translated into an identifier by which the operating system can refer to the process.
>
>Most processes that will be receiving connections are special-purpose **daemons**, which are system programs provided for that purpose. 
>
>***
>
>- In the shared-memory model
>  - processes use shared_memory_create() and shared_memory_attach() system calls to create and gain access to regions of memory owned by other processes. 

***

**Protection**

>Protection provides a mechanism for controlling access to the resources provided by a computer system. 

***

### 2.5 System Programs

> System programs, also known as **system utilities**, provide a convenient environment for program development and execution.
>
> Some of them are simply user interfaces to system calls. Others are considerably more complex. They can be divided into these categories:
>
> - **File management**. These programs create, delete, copy, rename, print, dump, list, and generally manipulate files and directories.
> - **Status information.** Some programs simply ask the system for the date, time, amount of available memory or disk space, number of users, or similar status information. Others are more complex, providing detailed performance, logging, and debugging information.
> - **File modification.** Several text editors may be available to create and modify the content of files stored on disk or other storage devices. 
> - **Programming-language support**. Compilers, assemblers, debuggers, and interpreters for common programming languages 
> - **Program loading and execution.** Once a program is assembled or compiled, it must be loaded into memory to be executed.
> - **Communications.** These programs provide the mechanism for creating virtual connections among processes, users, and computer systems. 
> - **Background services.** All general-purpose systems have methods for launching certain system-program processes at boot time. Constantly running system-program processes are known as **services**, **subsystems**, or **daemons.** 

***

### 2.6 Operating-System Design and Implementation

**Design Goals**

> At the highest level, the design of the system will be affected by the choice of hardware and the type of system: batch, time sharing, single user, multiuser, distributed, real time, or general purpose.
>
> The requirements can, however, be divided into two basic groups: **user goals** and **system goals**.

**Mechanisms and Policies**

> One important principle is the separation of **policy** from **mechanism**.
>
> Mechanisms determine ***how*** to do something; policies determine ***what*** will be done.
>
> The separation of policy and mechanism is important for flexibility. 
>
> Policies are likely to change across places or over time. In the worst case, each change in policy would require a change in the underlying mechanism. 
>
> A general mechanism insensitive to changes in policy would be more desirable. 

**Implementation**

***

### 2.7 Operating-System Structure

**question: how the probes work???**

>A system as large and complex as a modern operating system must be engineered carefully if it is to function properly and be modified easily. A common approach is to partition the task into small components, or modules, rather than have one monolithic system. 

**Layered Approach**

>A system can be made modular in many ways. One method is the layered approach, in which the operating system is broken into a number of layers (levels). The bottom layer (layer 0) is the hardware; the highest (layer N) is the user interface.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108095255424.png" alt="image-20201108095255424" style="zoom: 33%;" />

The main advantage of the layered approach is simplicity of construction and debugging. 

The layers are selected so that each uses functions (operations) and services of only lower-level layers. This approach simplifies debugging and system verification. 

>Each layer is implemented only with operations provided by lower-level layers. A layer does not need to know how these operations are implemented; it needs to know only what these operations do.

**Modules**

> Perhaps the best current methodology for operating-system design involves using ***loadable kernel modules***.
>
> Here, the kernel has a set of core components and links in additional services via modules, either at boot time or during run time. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108122746289.png" alt="image-20201108122746289" style="zoom:50%;" />

**Mac OS X**

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108123546257.png" alt="image-20201108123546257" style="zoom:50%;" />

***

### 2.8 Operating-System Debugging

**Failure Analysis**

> If a process fails, most operating systems write the error information to a ***log file*** to alert system operators or users that the problem occurred. 
>
> The operating system can also take a ***core dump***—a capture of the memory of the process—and store it in a file for later analysis. 
>
> A failure in the kernel is called a ***crash***. When a crash occurs, error information is saved to a log file, and the memory state is saved to a ***crash dump***.

- A kernel *failure in the file-system code* would make it risky for the kernel to try to save its state to a file on the file system before rebooting.
  - A common technique is to save the kernel’s memory state to ***a section of disk set aside for this purpose*** that contains no file system. If the kernel detects an unrecoverable error, it writes the entire contents of memory, or at least the kernel-owned parts of the system memory, to the disk area. 
  - Obviously, such strategies would be unnecessary for debugging ordinary user-level processes.

***

**Performance Tuning**

>We mentioned earlier that performance tuning seeks to improve performance by removing processing *bottlenecks*. 
>
>- To identify bottlenecks, we must be able to monitor system performance. 
>  - In a number of systems, the operating system does this by producing ***trace listings*** of system behavior. All interesting events are logged with their time and important parameters and are written to a file. 
>    - Later, an analysis program can process the log file to determine system performance and to identify bottlenecks and inefficiencies. 
>  - Another approach to performance tuning uses single-purpose, interactive tools that allow users and administrators to question the state of various system components to look for bottlenecks. 

***

**DTrace**

>DTrace is a facility that dynamically adds probes to a running system, both in user processes and in the kernel. These probes can be queried via the D programming language to determine an astonishing amount about the kernel, the system state, and process activities. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\T45M@LOT0N%W`[YENP0CZ0.png" alt="img" style="zoom:50%;" />

- In contrast, DTrace runs on production systems—systems that are running important or critical applications—and causes no harm to the system. It slows activities while enabled, but after execution it resets the system to its pre-debugging state.
- DTrace is composed of a compiler, a framework, ***providers*** of probes written within that framework, and ***consumers*** of those probes.

>DTrace providers create probes. 
>
>Kernel structures exist to keep track of all probes that the providers have created. The probes are stored in *a hash-table data structure* that is hashed by name and indexed according to unique probe identifiers.
>
>When a probe is enabled, a bit of code in the area to be probed is rewritten to call dtrace_probe (probe identifier) and then continue with the code’s original operation.
>
>***

> A DTrace consumer is code that is interested in a probe and its results.
>
> A consumer requests that the provider create one or more probes. When a probe fires, it emits data that are managed by the kernel. Within the kernel, actions called ***enabling control blocks***, or ECBs, are performed when probes fire. 
>
> Once the probe consumer terminates, its ECBs are removed. If there are no ECBs consuming a probe, the probe is removed. That involves rewriting the code to remove the dtrace_probe() call and put back the original code. 
>
> Thus, before a probe is created and after it is destroyed, the system is exactly the same, as if no probing occurred.

***

## 3.Processes

### 3.1 Process Concept

**The Process**

> Informally, a process is a program in execution.
>
> A process is more than the program code, which is sometimes known as the ***text section***.
>
> - It also includes the current activity, as represented by the value of the ***program counter*** and the contents of the processor’s registers. 
>
> - A process generally also includes the process ***stack***, which contains temporary data, and a ***data section***, which contains global variables. 
> - A process may also include a ***heap***, which is memory that is dynamically allocated during process run time. 

A program is a **passive** entity, In contrast, a process is an **active** entity, with a program counter specifying the next instruction to execute and a set of associated resources. 

A program becomes a process when an executable file is loaded into memory.

- Two common techniques for loading executable files are 
  - double-clicking an icon representing the executable file 
  - and entering the name of the executable file on the command line. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108224532445.png" alt="image-20201108224532445" style="zoom:33%;" />

>Although two processes may be associated with the same program, they are nevertheless considered two ***separate execution sequences***. 

***

**Process State**

>As a process executes, it changes ***state***. The state of a process is defined in part by the current activity of that process. A process may be in one of the following states:

- **New**. The process is being created.
- **Running**. Instructions are being executed.
- **Waiting**. The process is waiting for some event to occur (such as an I/O completion or reception of a signal).
- **Ready**. The process is waiting to be assigned to a processor.
- **Terminated**. The process has finished execution.

>It is important to realize that only one process can be running on any processor at any instant. Many
>processes may be ready and waiting, however.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108225151561.png" alt="image-20201108225151561" style="zoom:50%;" />

***

**Process Control Block**

>Each process is represented in the operating system by a ***process control block*** (PCB)—also called a ***task control block***. It contains many pieces of information associated with a specific process, including these:

- **Process state.** The state may be new, ready, running, waiting, halted, and so on.
- **Program counter.** The counter indicates the address of the next instruction to be executed for this process.
- **CPU registers.** The registers vary in number and type, depending on the computer architecture.Along with the program counter, this state information must be saved when an interrupt occurs, to allow the process to be continued correctly afterward.
- **CPU-scheduling information.** This information includes a process priority, pointers to scheduling queues, and any other scheduling parameters.
- **Memory-management information.** This information may include such items as the value of the base and limit registers and the page tables, or the segment tables, depending on the memory system used by the operating system.
- **Accounting information.** This information includes the amount of CPU and real time used, time limits, account numbers, job or process numbers, and so on.
- **I/O status information.** This information includes the list of I/O devices allocated to the process, a list of open files, and so on.

>In brief, the PCB simply serves as the repository for any information that may vary from process to process.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201108225915128.png" alt="image-20201108225915128" style="zoom:50%;" />

***

**Threads**

>The process model discussed so far has implied that a process is a program that performs a single ***thread*** of execution. 
>
>This single thread of control allows the process to perform only one task at a time.
>
>The user cannot simultaneously type in characters and run the spell checker within the same process.
>
>Most modern operating systems have extended the process concept to allow a process to have multiple threads of execution and thus to perform more than one task at a time.

***

> The process control block in the Linux operating system is represented by the C structure task struct, which is found in the ***<linux/sched.h>*** include file in the kernel source-code directory. This structure contains all the necessary information for representing a process, including the state of the process, scheduling and memory-management information, list of open files, and pointers to the process’s parent and a list of its children and siblings. Some of these fields include:

```c
long state; 					/* state of the process */
struct sched_entity se; 		/* scheduling information */
struct task_struct *parent; 	/* this process’s parent */
struct list_head children; 		/* this process’s children */
struct files_struct *files; 	/* list of open files */
struct mm_struct *mm; 			/* address space of this process */
```

> Within the Linux kernel, all active processes are represented using a doubly linked list of task_struct. The kernel maintains a pointer—current—to the process currently executing on the system, as shown below:

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\8WTO%P}5Q`NQ2S6YDBVBDI.png" alt="img" style="zoom:50%;" />

****

### 3.2 Process Scheduling

>The objective of multiprogramming is to *have some process running at all times*, to *maximize CPU utilization*. The objective of time sharing is to switch the CPU among processes so frequently that users can interact with each program while it is running. 
>
>To meet these objectives, the **process scheduler** selects an available process (possibly from a set of several available processes) for program execution on the CPU.

**Scheduling Queues**

>As processes enter the system, they are put into a ***job queue***, which consists of all processes in the system. 
>
>The processes that are residing in main memory and are ready and waiting to execute are kept on a list called the ***ready queue***.
>
>This queue is generally stored as a *linked list*. A ready-queue header contains pointers to the first and final PCBs in the list. Each PCB includes a pointer field that points to the next PCB in the ready queue.
>
>The list of processes waiting for a particular I/O device is called a ***device queue***. Each device has its own device queue.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\[SW7VU22898OKY1CC@%]7B.png" alt="img" style="zoom:50%;" />

> A new process is initially put in the ready queue. It waits there until it is selected for execution, or **dispatched**. Once the process is allocated the CPU and is executing, one of several events could occur:
>
> - The process could issue an I/O request and then be placed in an I/O queue.
> - The process could create a new child process and wait for the child’s termination.
> - The process could be removed forcibly from the CPU, as a result of an interrupt, and be put back in the ready queue.

**Schedulers**

>The operating system must select, for scheduling purposes, processes from these queues in some fashion. The selection process is carried out by the appropriate **scheduler.**
>
>Often, in a batch system, more processes are submitted than can be executed immediately. These processes are spooled to a ***mass-storage device*** (typically a disk), where they are kept for later execution. 
>
>The **long-term scheduler**, or job scheduler, selects processes from this pool and loads them into memory for execution. 
>
>The **short-term scheduler**, or CPU scheduler, selects from among the processes that are ready to execute and allocates the CPU to one of them.

- The primary distinction between these two schedulers lies in ***frequency*** of execution. 
  - The short-term scheduler must select a new process for the CPU frequently. 
  - A process may execute for only a few milliseconds before waiting for an I/O request. 
  - Because of the short time between executions, the short-term scheduler must be fast. 
- The long-term scheduler executes much less frequently
  - minutes may separate the creation of one new process and the next. 
  - The long-term scheduler controls the **degree of multiprogramming** (the number of processes in memory). 
- In general, most processes can be described as either I/O bound or CPU bound.
  - An **I/O-bound** process is one that spends more of its time doing I/O than it spends doing computations. 
  - A **CPU-bound** process, in contrast, generates I/O requests infrequently, using more of its time doing computations. 
- This **medium-term scheduler** is diagrammed in Figure 3.7. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201113214701206.png" alt="image-20201113214701206" style="zoom:50%;" />

***

**Context Switch**

>When an interrupt occurs, the system needs to save the current ***context*** of the process running on the CPU.
>
>Generically, we perform a ***state save*** of the current state of the CPU, be it in kernel or user mode, and then a ***state restore*** to resume operations.

- Switching the CPU to another process requires performing a state save of the current process and a state restore of a different process. This task is known as a **context switch**. 

***

### 3.3 Operations on Processes

**Process Creation**

> During the course of execution, a process may create several new processes. 
>
> Each of these new processes may in turn create other processes, forming a **tree** of processes.
>
> Most operating systems (including UNIX, Linux, and Windows) identify processes according to a unique process identifier (or ***pid***), which is typically an integer number. 
>
> 

![image-20201113220754861](C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201113220754861.png)

- The ***init*** process (which always has a pid of 1) serves as the ***root parent process*** for all user processes.

  >Once the system has booted, the init process can also create various user processes, such as a web or print server, an ssh server, and the like. 

- We see two children of init—***kthreadd*** and ***sshd***. 

  - The kthreadd process is responsible for creating additional processes that perform tasks on behalf of the kernel (in this situation, khelper and pdflush). 
  - The sshd process is responsible for managing clients that connect to the system by using ssh (which is short for ***secure shell***).

- The ***login*** process is responsible for managing clients that directly log onto the system.

>On UNIX and Linux systems, we can obtain a listing of processes by using the **ps** command.

When a process creates a new process, two possibilities for execution exist:
- The parent continues to execute concurrently with its children.

- The parent waits until some or all of its children have terminated.

There are also two address-space possibilities for the new process:
- The child process is a duplicate of the parent process (it has the same program and data as the parent).

- The child process has a new program loaded into it.

>Both processes (the parent and the child) continue execution at the instruction after the ***fork()***, with one difference: 
>
>the return code for the fork() is zero for the new (child) process, whereas *the (nonzero) process identifier of the child* is returned to the parent.
>
>After a fork() system call, one of the two processes typically uses the ***exec()*** system call to ***replace the process’s memory space with a new program***. The exec() system call *loads a binary file* into memory (destroying the memory image of the program containing the exec() system call) and starts
>its execution. 
>
>The parent can then create more children; or, if it has nothing else to do while the child runs, it can issue a ***wait()*** system call to move itself off the ready queue until the termination of the child. 

```c
#include <sys/types.h>
#include <stdio.h>
#include <unistd.h>
int main()
{
pid_t pid;
    
    /* fork a child process */
    pid = fork();
    
    if (pid < 0) { 		/* error occurred */
        fprintf(stderr, "Fork Failed");
        return 1;
    }
    else if (pid == 0) { /* child process */
    	execlp("/bin/ls","ls",NULL);
    }
    else { /* parent process */
        /* parent will wait for the child to complete */
        wait(NULL);
        printf("Child Complete");
    }
    
    return 0;
}
```

![image-20201113224305769](C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201113224305769.png)

> The C program shown in Figure 3.11 illustrates the CreateProcess() function, which creates a child process that loads the application mspaint.exe.

```c
#include <stdio.h>
#include <windows.h>
int main(VOID)
{
STARTUPINFO si;
PROCESS_INFORMATION pi;
    /* allocate memory */
    ZeroMemory(&si, sizeof(si));
    si.cb = sizeof(si);
    ZeroMemory(&pi, sizeof(pi));
    
    /* create child process */
    if (!CreateProcess(NULL, /* use command line */
        "C:\\WINDOWS\\system32\\mspaint.exe", /* command */
        NULL, /* don’t inherit process handle */
        NULL, /* don’t inherit thread handle */
        FALSE, /* disable handle inheritance */
        0, /* no creation flags */
        NULL, /* use parent’s environment block */
        NULL, /* use parent’s existing directory */
        &si,
        &pi))
    {
        fprintf(stderr, "Create Process Failed");
        return -1;
    }
    
    /* parent will wait for the child to complete */
    WaitForSingleObject(pi.hProcess, INFINITE);
    printf("Child Complete");
    
    /* close handles */
    CloseHandle(pi.hProcess);
    CloseHandle(pi.hThread);
}
```

***

**Process Termination**

>A process terminates when it finishes executing its final statement and asks the operating system to delete it by using the exit() system call. At that point, the process may return a status value (typically an integer) to its parent process (via the wait() system call).
>
>A process can cause the termination of another process via an appropriate system call.
>
>A parent may terminate the execution of one of its children for a variety of reasons, such as these:
>
>- The child has exceeded its usage of some of the resources that it has been allocated. (To determine whether this has occurred, the parent must have a mechanism to inspect the state of its children.)
>
>- The task assigned to the child is no longer required.
>
>- The parent is exiting, and the operating system does not allow a child to continue if its parent terminates. 
>    - This phenomenon, referred to as **cascading termination**, is normally initiated by the operating system.

- A parent process may wait for the termination of a child process by using the wait() system call. 
  - The wait() system call is passed a parameter that allows the parent to obtain the exit status of the child. 
  - This system call also returns the process identifier of the terminated child so that the parent can tell
    which of its children has terminated:

>Now consider what would happen if a parent did not invoke wait() and instead terminated, thereby leaving its child processes as ***orphans***.

- Linux and UNIX address this scenario by assigning the init process as the new parent to orphan processes. 
- The init process periodically invokes wait(), thereby allowing the exit status of any orphaned process to be collected and releasing the orphan’s process identifier and process-table entry.

****

### 3.4 Interprocess Communication

>A process is ***independent*** if it cannot affect or be affected by the other processes executing in the system.
>
>A process is ***cooperating*** if it can affect or be affected by the other processes executing in the system.

- There are several reasons for providing an environment that allows process cooperation:
  - ***Information sharing.*** Several users may be interested in the same piece of information (for instance, a shared file).
  - ***Computation speedup.*** If we want a particular task to run faster, we must break it into subtasks, each of which will be executing in parallel with the others. 
  - ***Modularity.*** We may want to construct the system in a modular fashion, dividing the system functions into separate processes or threads.
  - ***Convenience.*** Even an individual user may work on many tasks at the same time. 

>Cooperating processes require an ***interprocess communication*** (***IPC***) mechanism that will allow them to exchange data and information.
>
>There are two fundamental models of interprocess communication: 
>
>**shared memory** and **message passing**.
>
>- In the shared-memory model, a region of memory that is shared by cooperating processes is established. 
>  - Processes can then exchange information by reading and writing data to the shared region. 
>- In the message-passing model, communication takes place by means of messages exchanged between the cooperating processes. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201115102351001.png" alt="image-20201115102351001" style="zoom:50%;" />

***

**Shared-Memory Systems**

>To illustrate the concept of cooperating processes, let’s consider the producer–consumer problem.
>
>A producer process produces information that is consumed by a consumer process. For example, a compiler may produce assembly code that is consumed by an assembler. 
>
>One solution to the producer–consumer problem uses shared memory. 
>
>- To allow producer and consumer processes to run concurrently, we must haveavailable a buffer of items that can be filled by the producer and emptied by the consumer. 
>- This buffer will reside in a region of memory that is shared by the producer and consumer processes.
>- Two types of buffers can be used. 
>  - The ***unbounded buffer*** places no practical limit on the size of the buffer. The consumer may have to wait for new items, but the producer can always produce new items. 
>  - The ***bounded buffer*** assumes a fixed buffer size. In this case, the consumer must wait if the buffer is empty, and the producer must wait if the buffer is full.

****

**Message-Passing Systems**

> Message passing may be either ***blocking*** or ***nonblocking***— also known as ***synchronous*** and ***asynchronous***. 
>
> - **Blocking send**. The sending process is blocked until the message is received by the receiving process or by the mailbox.
> - **Nonblocking send.** The sending process sends the message and resumes operation.
> - **Blocking receive.** The receiver blocks until a message is available.
> - **Nonblocking receive.** The receiver retrieves either a valid message or a null.

***

### 3.5 Examples of IPC Systems

**An Example: POSIX Shared Memory**

>POSIX shared memory is organized using ***memory-mapped files***, which associate the region of shared memory with a file. A process must first create a shared-memory object using the ***shm_open()*** system call, as follows:
>
>```c
>shm_fd = shm_open(name, O_CREAT | O_RDRW, 0666);
>```
>
>- The first parameter specifies the ***name*** of the shared-memory object. Processes that wish to access this shared memory must refer to the object by this name.
>- The subsequent parameters specify that the shared-memory object is to be created if it does not yet exist (O_CREAT) and that the object is open for reading and writing (O_RDRW). 
>- The last parameter establishes the directory ***permissions*** of the shared-memory object. 
>- A successful call to shm_open() returns an ***integer file descriptor*** for the shared-memory object.

- Once the object is established, the ***ftruncate()*** function is used to configure the size of the object in bytes. 

```c
ftruncate(shm_fd, 4096);
```

- Finally, the ***mmap()*** function establishes a memory-mapped file containing the shared-memory object. It also returns a pointer to the memory-mapped file that is used for accessing the shared-memory object.

```c
#include <stdio.h>
#include <stlib.h>
#include <string.h>
#include <fcntl.h>
#include <sys/shm.h>
#include <sys/stat.h>
int main()
{
    /* the size (in bytes) of shared memory object */
    const int SIZE 4096;
    /* name of the shared memory object */
    const char *name = "OS";
    /* strings written to shared memory */
    const char *message_0 = "Hello";
    const char *message_1 = "World!";
    /* shared memory file descriptor */
    int shm_fd;
    /* pointer to shared memory obect */
    void *ptr;
    /* create the shared memory object */
    shm_fd = shm_open(name, O_CREAT | O_RDRW, 0666);
    /* configure the size of the shared memory object */
    ftruncate(shm_fd, SIZE);
    /* memory map the shared memory object */
    ptr = mmap(0, SIZE, PROT_WRITE, MAP_SHARED, shm_fd, 0);
    /* write to the shared memory object */
    sprintf(ptr,"%s",message_0);
    ptr += strlen(message_0);
    sprintf(ptr,"%s",message_1);
    ptr += strlen(message_1);
    return 0;
}
```

```c
#include <stdio.h>
#include <stlib.h>
#include <fcntl.h>
#include <sys/shm.h>
#include <sys/stat.h>
int main()
{
    /* the size (in bytes) of shared memory object */
    const int SIZE 4096;
    /* name of the shared memory object */
    const char *name = "OS";
    /* shared memory file descriptor */
    int shm_fd;
    /* pointer to shared memory obect */
    void *ptr;
    /* open the shared memory object */
    shm_fd = shm_open(name, O_RDONLY, 0666);
    /* memory map the shared memory object */
    ptr = mmap(0, SIZE, PROT_READ, MAP_SHARED, shm_fd, 0);
    /* read from the shared memory object */
    printf("%s",(char *)ptr);
    /* remove the shared memory object */
    shm_unlink(name);
    return 0;
}
```

>Even system calls are made by messages. When a task is created, two special mailboxes—the Kernel mailbox and the Notify mailbox—are also created.
>
>The kernel uses the Kernel mailbox to communicate with the task and sends notification of event occurrences to the Notify port. Only three system calls are needed for message transfer. 
>
>- The msg_send() call sends a message to a mailbox. 
>
>- A message is received via msg_receive(). 
>- Remote procedure calls (RPCs) are executed via msg_rpc(), which sends a message and waits for exactly one return message from the sender. 

***

**An Example: Windows**

> The message-passing facility in Windows is called the ***advanced local procedure call*** (ALPC) facility. 
>
> Windows uses a port object to establish and maintain a connection between two processes. Windows uses two types of ports: ***connection ports*** and ***communication ports.***
>
> - Server processes publish connection-port objects that are visible to all processes. 
>   - When a client wants services from a subsystem, it opens a handle to the server’s connection-port object and sends a connection request to that port.
>   - The server then creates a channel and returns a handle to the client. 
>   - The channel consists of a pair of private communication ports: 
>     - one for client—server messages,
>     - the other for server—client messages.
>   - Additionally, communication channels support a ***callback mechanism*** that allows the client and server to accept requests when they would normally be expecting a reply.
> - When an ALPC channel is created, one of three message-passing techniques is chosen:
>   1. For small messages (up to 256 bytes), the port’s message queue is used as intermediate storage, and the messages are copied from one process to the other.
>   2. Larger messages must be passed through a section object, which is a region of shared memory associated with the channel.
>   3. When the amount of data is too large to fit into a section object, an API is available that allows server processes to read and write directly into the address space of a client.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116102604656.png" alt="image-20201116102604656" style="zoom:50%;" />

***

### 3.6 Communication in Client–Server Systems

**Sockets**

> A socket is defined as an endpoint for communication. 

**Remote Procedure Calls**

> A port is simply a number included at the start of a message packet.

- The RPC system hides the details that allow communication to take place by providing a **stub** on the client side.

  >A separate stub exists for each separate remote procedure. 
  >
  >When the client invokes a remote procedure, the RPC system calls the appropriate stub, passing it the parameters provided to the remote procedure. 
  >
  >This stub locates the port on the server and **marshals** the parameters. Parameter marshalling involves packaging the parameters into a form that can be transmitted over a network. 
  >
  >The stub then transmits a message to the server using message passing. 
  >
  >A similar stub on the server side receives this message and invokes the procedure on the server.

- One issue that must be dealt with concerns differences in data representation on the client and server machines. 
  
  - To resolve differences like this, many RPC systems define a machine-independent representation of data. One such representation is known as ***external data representation*** (**XDR**). 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\LHONPZ07RE4}Y5F49Y6_$UD.png" alt="img" style="zoom:50%;" />

****

**Pipes**

> A pipe acts as a conduit allowing two processes to communicate. 
>
> In implementing a pipe, four issues must be considered:	
>
> 1. Does the pipe allow ***bidirectional communication***, or is communication unidirectional?
> 2. If two-way communication is allowed, is it ***half duplex*** (data can travel only one way at a time) or ***full duplex*** (data can travel in both directions at the same time)?
> 3. Must a ***relationship*** (such as parent–child) exist between the communicating processes?
> 4. Can the pipes ***communicate over*** a network, or must the communicating processes reside on the same machine?

**Ordinary Pipes**

>Ordinary pipes allow two processes to communicate in standard producer–consumer fashion: 
>
>the producer writes to one end of the pipe (the **write-end**) and the consumer reads from the other end (the **read-end**). 

- As a result, ordinary pipes are unidirectional, allowing only one-way communication.

- On UNIX systems, ordinary pipes are constructed using the function

  ```c
  pipe(int fd[]);
  ```

  - This function creates a pipe that is accessed through the **int fd[]** file descriptors: 
  - fd[0] is the read-end of the pipe, and fd[1] is the write-end.
  - UNIX treats a pipe as a special type of file. Thus, pipes can be accessed using ordinary read() and write() system calls.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116111311170.png" alt="image-20201116111311170" style="zoom:50%;" />

>Note that ordinary pipes require a parent–child relationship between the communicating processes on both UNIX and Windows systems. This means that these pipes can be used only for communication between processes on the same machine.

**Named Pipes**



****

## 4.Thread

### 4.1 Overview

>A thread is a basic unit of CPU utilization; 
>
>it comprises a thread ID, a program counter, a register set, and a stack. 
>
>It shares with other threads belonging to the same process its code section, data section, and other operating-system resources, such as open files and signals. 
>
>A traditional (or heavyweight) process has a single thread of control. If a process has multiple threads of control, it can perform more than one task at a time. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116123416523.png" alt="image-20201116123416523" style="zoom:50%;" />

**Motivation**

>Most software applications that run on modern computers are multithreaded.
>An application typically is implemented as a separate process with several threads of control. 

***

### 4.2 Multicore Programming

**Programming Challenges**

> The trend towards multicore systems continues to place pressure on system designers and application programmers to make better use of the multiple computing cores. 
>
> For application programmers, the challenge is to modify existing programs as well as design new programs that are multithreaded.
>
> In general, five areas present challenges in programming for multicore systems:
>
> - **Identifying tasks.** This involves examining applications to find areas that can be divided into separate, concurrent tasks. Ideally, tasks are independent of one another and thus can run in parallel on individual cores.
> - **Balance.** Programmers must also ensure that the tasks perform equal work of equal value. Using a separate execution core to run that task may not be worth the cost.
> - **Data splitting.** Just as applications are divided into separate tasks, the data accessed and manipulated by the tasks must be divided to run on separate cores.
> - **Data dependency**. The data accessed by the tasks must be examined for dependencies between two or more tasks. 
> - **Testing and debugging.** When a program is running in parallel on multiple cores, many different execution paths are possible. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116145200668.png" alt="image-20201116145200668" style="zoom:50%;" />

***

**Types of Parallelism**

- In general, there are two types of parallelism: data parallelism and task parallelism. 

  - **Data parallelism** focuses on distributing subsets of the same data across multiple computing cores and performing the same operation on each core.

    >On a dual-core system, however, thread A, running on core 0, could sum the elements [0] . . . [N/2 − 1] while thread B, running on core 1, could sum the elements [N/2] . . . [N − 1]. The two threads would be running in parallel on separate computing cores.

  - **Task parallelism** involves distributing not data but tasks (threads) across multiple computing cores. Each thread is performing a unique operation. Different threads may be operating on the same data, or they may be operating on different data. 

- In most instances, applications use a hybrid of these two strategies.

****

### 4.3 Multithreading Models

>Support for threads may be provided either at the user level, for **user threads**, or by the kernel, for **kernel threads**. 
>
>User threads are supported above the kernel and are managed without kernel support, whereas kernel threads are supported and managed directly by the operating system. 
>
>A relationship must exist between user threads and kernel threads. In this section, we look at three common ways of establishing such a relationship: 
>
>the many-to-one model, the one-to-one model, and the many-to-many model.

****

**Many-to-One Model**

>The many-to-one model (Figure 4.5) maps many user-level threads to one kernel thread. 
>
>Thread management is done by the thread library in user space, so it is efficient.
>
>However, the entire process will block if a thread makes a blocking system call. 
>
>Also, because only one thread can access the kernel at a time, multiple threads are unable to run in parallel on multicore systems.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116152547846.png" alt="image-20201116152547846" style="zoom:50%;" />

**One-to-One Model**

>The one-to-one model (Figure 4.6) maps each user thread to a kernel thread. 
>
>It provides more concurrency than the many-to-one model by allowing another thread to run when a thread makes a blocking system call. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116152618042.png" alt="image-20201116152618042" style="zoom:50%;" />

**Many-to-Many Model**

>The many-to-many model (Figure 4.7) multiplexes many user-level threads to a smaller or equal number of kernel threads.
>
>The number of kernel threads may be specific to either a particular application or a particular machine.

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201116152921597.png" alt="image-20201116152921597" style="zoom:50%;" />

***

### 4.4 Thread Libraries

>A **thread library** provides the programmer with an API for creating and managing threads. 
>
>There are two primary ways of implementing a thread library. 
>
>- The first approach is to provide a library entirely in user space with no kernel support. 
>  - This means that invoking a function in the library results in a local function call in user space and not a system call.
>- The second approach is to implement a kernel-level library supported directly by the operating system. 

- Three main thread libraries are in use today: POSIX Pthreads, Windows, and Java. 
  - **Pthreads**, the threads extension of the POSIX standard, may be provided as either a user-level or a kernel-level library. 
    - UNIX and Linux systems often use Pthreads.
  - The **Windows thread** library is a kernel-level library available on Windows systems. 
  - The **Java thread** API allows threads to be created and managed directly in Java programs. 
    - Because in most instances the JVM is running on top of a host operating system, the Java thread API is generally implemented using a thread library available on the host system. 

___

- For POSIX and Windows threading, any data ***declared globally***—that is, declared ***outside of any function***—are shared among all ***threads belonging to the same process***. 
- Because Java has no notion of global data, access to shared data must be explicitly arranged between threads. 
  - Data declared local to a function are typically stored on the stack. Since each thread has its own stack, each thread has its own copy of local data.
- There are two general strategies for creating multiple threads: **asynchronous threading** and **synchronous threading**.
  - With asynchronous threading, once the parent creates a child thread, the parent resumes its execution, so that the parent and child execute concurrently. 
  - Synchronous threading occurs when the parent thread creates one or more children and then must wait for all of its children to terminate before it resumes—the so-called **fork-join** strategy. 
  - Once each thread has finished its work, it terminates and joins with its parent. 
  - Only after all of the children have joined can the parent resume execution. 

***

**Pthreads**

>Pthreads refers to the POSIX standard (IEEE 1003.1c) defining an API for thread creation and synchronization. This is a specification for thread behavior, not an implementation. 

The C program shown below demonstrates the basic Pthreads API for constructing a multithreaded program that calculates the summation of a non-negative integer in a separate thread. 

```c
#include <pthread.h>
#include <stdio.h>
int sum; /* this data is shared by the thread(s) */
void *runner(void *param); /* threads call this function */
int main(int argc, char *argv[])
{
    pthread_t tid; /* the thread identifier */
    pthread_attr_t attr; /* set of thread attributes */
    if (argc != 2) {
        fprintf(stderr,"usage: a.out <integer value>\n");
        return -1;
    }
    if (atoi(argv[1]) < 0) {
        fprintf(stderr,"%d must be >= 0\n",atoi(argv[1]));
        return -1;
    }
    /* get the default attributes */
    pthread_attr_init(&attr);
    /* create the thread */
    pthread_create(&tid, &attr, runner, argv[1]);
    /* wait for the thread to exit */
    pthread_join(tid,NULL);
    printf("sum = %d\n",sum);
}

/* The thread will begin control in this function */
void *runner(void *param)
{
    int i, upper = atoi(param);
    sum = 0;
    for (i = 1; i <= upper; i++)
        sum += i;
    pthread_exit(0);
}
```

- In a Pthreads program, separate threads begin execution in a specified function. 
  - In code aobve, this is the runner() function. 
  - When this program begins, a single thread of control begins in main(). 
  - After some initialization, main() creates a second thread that begins control in the runner() function. Both threads share the global data sum.
- All Pthreads programs must include the pthread.h header file. The statement pthread_t tid declares the identifier for the thread we will create. 
- Each thread has a set of attributes, including stack size and scheduling information. The pthread_attr_t attr declaration represents the attributes for the thread. 
  - Because we did not explicitly set any attributes, we use the default attributes provided.
- In addition to passing the thread identifier and the attributes for the thread, we also pass the name of the function where the new thread will begin execution—in this case, the runner() function.

- Last, we pass the integer parameter that was provided on the command line, argv[1].

>This program follows the **fork-join** strategy described earlier: after creating the summation thread, the parent thread will wait for it to terminate by calling the pthread_join() function. 
>
>The summation thread will terminate when it calls the function pthread_exit().

****

**Windows Threads**

````c
#include <windows.h>
#include <stdio.h>
DWORD Sum; /* data is shared by the thread(s) */
/* the thread runs in this separate function */
DWORD WINAPI Summation(LPVOID Param)
{
    DWORD Upper = *(DWORD*)Param;
    for (DWORD i = 0; i <= Upper; i++)
    	Sum += i;
    return 0;
}
int main(int argc, char *argv[])
{
    DWORD ThreadId;
    HANDLE ThreadHandle;
    int Param;
    if (argc != 2) {
        fprintf(stderr,"An integer parameter is required\n");
        return -1;
    }
    Param = atoi(argv[1]);
    if (Param < 0) {
        fprintf(stderr,"An integer >= 0 is required\n");
        return -1;
    }
    
    /* create the thread */
    ThreadHandle = CreateThread(
        NULL, 		/* default security attributes */
        0, 			/* default stack size */
        Summation, 	/* thread function */
        &Param, 	/* parameter to thread function */
        0, 			/* default creation flags */
        &ThreadId); /* returns the thread identifier */
    if (ThreadHandle != NULL) {
        /* now wait for the thread to finish */
        WaitForSingleObject(ThreadHandle,INFINITE);
        /* close the thread handle */
        CloseHandle(ThreadHandle);
        printf("sum = %d\n",Sum);
    }
}
````

***

**Java Threads**



***

### 4.5 Implicit Threading

> With the continued growth of multicore processing, applications containing hundreds—or even thousands—of threads are looming on the horizon.
> Designing such applications is not a trivial undertaking.
>
> One way to address these difficulties and better support the design of multithreaded applications is to transfer the creation and management of threading from application developers to compilers and run-time libraries. 
>
> This strategy, termed **implicit threading**, is a popular trend today. 

**Thread Pools**

Consider this situation:

- Whenever the server receives a request, it creates a separate thread to service the request. 
- The ***first issue*** concerns the amount of **time** required to create the thread, together with the fact that the thread will be discarded once it has completed its work. 
- The ***second issue*** is more troublesome. If we allow all concurrent requests to be serviced in a new thread, we have not placed a **bound** on the number of threads concurrently active in the system. 

>Unlimited threads could exhaust system resources, such as CPU time or memory. One solution to this problem is to use a ***thread pool***.
>
>The general idea behind a thread pool is to create a number of threads at process **startup** and place them into a pool, where they sit and wait for work.

- Thread pools offer these ***benefits***:
  - Servicing a request with an existing thread is ***faster*** than waiting to create a thread.
  - A thread pool ***limits*** the number of threads that exist at any one point. This is particularly important on systems that cannot support a large number of concurrent threads.
  - ***Separating*** the task to be performed from the mechanics of creating the task allows us to use different strategies for running the task. 

- The Windows API provides several functions related to thread pools. 
  - Here, a function that is to run as a separate thread is defined. 
  - A pointer to PoolFunction() is passed to one of the functions in the thread pool API
  - and a thread from the pool executes this function.
  - One such member in the thread pool API is the QueueUserWorkItem() function, which is passed
    three parameters:
    - LPTHREAD_START_ROUTINE Function—a pointer to the function that is to run as a separate thread.
    - PVOID Param—the parameter passed to Function.
    - ULONG Flags—flags indicating how the thread pool is to create and manage execution of the thread.

```c
DWORD WINAPI PoolFunction(AVOID Param) {
    /*
    * this function runs as a separate thread.
    */
}
```

- An example of invoking a function is the following:

  ```c
  QueueUserWorkItem(&PoolFunction, NULL, 0);
  ```

  This causes a thread from the thread pool to invoke PoolFunction() on behalf of the programmer. In this instance, we pass no parameters to PoolFunction(). 

  Because we specify 0 as a flag, we provide the thread pool with no special instructions for thread creation.

***

**OpenMP**

>OpenMP is a set of compiler directives as well as an API for programs written in C, C++, or FORTRAN that provides support for ***parallel programming*** in shared-memory environments. 
>
>OpenMP identifies **parallel regions** as blocks of code that may run in parallel. 
>
>Application developers insert compiler directives into their code at parallel regions, and these directives instruct the OpenMP run-time library to execute the region in parallel. 

- The following C program illustrates a compiler directive above the parallel region containing the printf() statement:

```c
#include <omp.h>
#include <stdio.h>
int main(int argc, char *argv[])
{
    /* sequential code */
    #pragma omp parallel
    {
    	printf("I am a parallel region.");
    }
    /* sequential code */
    return 0;
}
```

- When OpenMP encounters the directive
  `#pragma omp parallel`

  - it creates ***as many threads are there are processing cores*** in the system. Thus, for a dual-core system, two threads are created, for a quad-core system, four are created; and so forth. 

  - All the threads then simultaneously execute the parallel region. As each thread exits the parallel region, it is terminated.

***

**Grand Central Dispatch**





***

### 4.6 Threading Issues

>In this section, we discuss some of the issues to consider in designing multithreaded programs.

**The fork() and exec() System Calls**

>The semantics of the fork() and exec() system calls change in a multithreaded program.

***

### 4.7 Operating-System Examples

**Windows Threads**

> A Windows application runs as a separate process, and each process may contain one or more threads. 

- The general components of a thread include:
  - A ***thread ID*** uniquely identifying the thread
  - A ***register set*** representing the status of the processor
  - A ***user stack***, employed when the thread is running in user mode, and a kernel stack, employed when the thread is running in kernel mode
  - A ***private storage area*** used by various run-time libraries and dynamic link libraries (DLLs)

- The register set, stacks, and private storage area are known as the **context** of the thread.
- The primary data structures of a thread include:
  - ETHREAD—executive thread block
  - KTHREAD—kernel thread block
  - TEB—thread environment block

>The key components of the ETHREAD include a pointer to the process to which the thread belongs and the address of the routine in which the thread starts control. 
>
>The ETHREAD also contains a pointer to the corresponding KTHREAD.
>
>The KTHREAD includes scheduling and synchronization information for the thread. In addition, the KTHREAD includes the kernel stack (used when the thread is running in kernel mode) and a pointer to the TEB.
>
>The ETHREAD and the KTHREAD exist entirely in kernel space; this means that only the kernel can access them. 
>
>The TEB is a user-space data structure that is accessed when the thread is running in user mode. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\1.png" style="zoom:50%;" />

***

## 5.Process Synchronization

>A **cooperating process** is one that can affect or be affected by other processes executing in the system. 
>
>In this chapter, we discuss various mechanisms to ensure the orderly execution of cooperating processes that share a logical address space, so that data consistency is maintained.

### 5.1 Background

>We would arrive at this incorrect state because we allowed both processes to manipulate the variable counter concurrently. A situation like this, where several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place, is called a **race condition**. (竞态条件)
>
>To guard against the race condition above, we need to ensure that only one process at a time can be manipulating the variable. 
>
>To make such a guarantee, we require that the processes be synchronized in some way.

***

### 5.2 The Critical-Section Problem

> Consider a system consisting of n processes {P0, P1, ..., Pn−1}. Each process has a segment of code, called a ***critical section***, in which the process may be *changing common variables, updating a table, writing a file*, and so on. 
>
> The important feature of the system is that, when one process is executing in its critical section, no other process is allowed to execute in its critical section. 
>
> The critical-section problem is to design a protocol that the processes can use to cooperate. 
>
> Each process must ***request permission to enter*** its critical section. The section of code implementing this request is the **entry section**. 
>
> The critical section may be followed by an **exit section**. 
>
> The remaining code is the **remainder section**. 

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201118090417702.png" alt="image-20201118090417702" style="zoom:50%;" />

- A solution to the critical-section problem must satisfy the following three requirements:
  - **Mutual exclusion**. If process P~i~ is executing in its critical section, then no other processes can be executing in their critical sections.
  - **Progress**. If no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next, and this selection cannot be postponed indefinitely.
  - **Bounded waiting.** There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections.

- Two general approaches are used to handle critical sections in operating systems: 
  - **preemptive kernels** and **nonpreemptive kernels**. 
  - A preemptive kernel allows a process to be preempted(抢占) while it is running in kernel mode. 
  - A nonpreemptive kernel does not allow a process running in kernel mode to be preempted; a kernel-mode process will run until it exits kernel mode, blocks, or voluntarily yields control of the CPU.

***

### 5.3 Peterson’s Solution

<img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201118093558519.png" alt="image-20201118093558519" style="zoom:50%;" />

> Peterson’s solution is restricted to two processes that alternate execution between their critical sections and remainder sections. 
>
> The variable **turn** indicates whose turn it is to enter its critical section. That is, if turn == i, then process P~i~ is allowed to execute in its critical section. 
>
> The **flag** array is used to indicate if a process is ready to enter its critical section.
> For example, if flag[i] is true, this value indicates that P~i~ is ready to enter its critical section. 

***

### 5.4 Synchronization Hardware

>In the following discussions, we explore several more solutions to the critical-section problem using techniques ranging from hardware to software-based APIs available to both kernel developers and application programmers. 
>
>All these solutions are based on the premise of **locking** —that is, protecting critical regions through the use of **locks**. 

<img src=".\image-20201118100054570.png" alt="image-20201118100054570" style="zoom:50%;" />

<img src=".\image-20201118100109614.png" alt="image-20201118100109614" style="zoom:50%;" />

<img src=".\image-20201118100139246.png" alt="image-20201118100139246" style="zoom:50%;" />

- The important characteristic of this instruction is that it is executed atomically.
  - Thus, if two ***test_and_set()*** instructions are executed simultaneously (each on a different CPU), they will be executed sequentially in some arbitrary order.

- If the machine supports the test_and_set() instruction, then we can implement mutual exclusion by declaring a boolean variable lock, initialized to false.

***

- The ***compare_and_swap()*** instruction, in contrast to the test_and_set() instruction, operates on three operands.

<img src=".\image-20201118101100034.png" alt="image-20201118101100034" style="zoom:50%;" /><img src="C:\Users\13793\Desktop\学习笔记\操作系统概念\image-20201118101240271.png" alt="image-20201118101240271" style="zoom:50%;" />

<img src=".\image-20201118101849542.png" alt="image-20201118101849542" style="zoom:50%;" />

>Mutual exclusion can be provided as follows: a global variable (lock) is declared and is initialized to 0. 
>
>The first process that invokes compare_and_swap() will set lock to 1. 
>
>Subsequent calls to compare_and_swap() will not succeed, because lock now is not equal to the expected value of 0. 
>
>When a process exits its critical section, it sets lock back to 0, which allows another process to enter its critical section.

****

### 5.5 Mutex Locks

> The simplest of these tools is the mutex lock. (In fact, the term mutex is short for mutual exclusion.)
>
> We use the mutex lock to protect critical regions and thus prevent race conditions. 
>
> That is, a process must acquire the lock before entering a critical section; it releases the lock when it exits the critical section.  
>
> The acquire()function acquires the lock, and the release() function releases the lock

```c
acquire() {
    while (!available)
    ; /* busy wait */
    available = false;;
}

do {
    acquire lock
    	critical section
    release lock
    	remainder section
} while (true);
```

- A mutex lock has a boolean variable available whose value indicates if the lock is available or not. 

  A process that attempts to acquire an unavailable lock is blocked until the lock is released.

>The main disadvantage of the implementation given here is that it requires **busy waiting**. 
>
>While a process is in its critical section, any other process that tries to enter its critical section must ***loop continuously*** in the call to acquire().
>
>This continual looping is clearly a problem in a real multiprogramming system, where a single CPU is shared among many processes. 
>
>Busy waiting ***wastes CPU cycles*** that some other process might be able to use productively.

***

### 5.6 Semaphores

>A **semaphore** S is an integer variable that, apart from initialization, is accessed only through two standard atomic operations: wait() and signal().

- The definition of wait() is as follows:

````c
wait(S) {
    while (S <= 0)
    ; // busy wait
    S--;
}
````

- The definition of signal() is as follows:

```c
signal(S) {
	S++;
}
```

- All modifications to the integer value of the semaphore in the wait() and signal() operations must be executed indivisibly. 

***

**Semaphore Usage**

>Operating systems often distinguish between counting and binary semaphores.
>
>The value of a **counting semaphore** can range over an unrestricted domain. 
>
>The value of a **binary semaphore** can range only between 0 and 1. 

- Counting semaphores can be used to control access to a given resource consisting of a finite number of instances. 
  - The semaphore is initialized to the number of resources available. 
  - Each process that wishes to use a resource performs a wait() operation on the semaphore (thereby decrementing the count). 
  - When the count for the semaphore goes to 0, all resources are being used. After that, processes that wish to use a resource will block until the count becomes greater than 0.

***

**Semaphore Implementation**

>To overcome the need for busy waiting, we can modify the definition of the wait() and signal() operations as follows: 
>
>- When a process executes the wait() operation and finds that the semaphore value is not positive, it must wait. 
>- However, rather than engaging in busy waiting, the process can block itself. 
>- The block operation places a process into a waiting queue associated with the semaphore, and the
>  state of the process is switched to the waiting state. 
>- Then control is transferred to the CPU scheduler, which selects another process to execute.

**Deadlocks and Starvation**

>The implementation of a semaphore with a waiting queue may result in a situation where two or more processes are ***waiting indefinitely*** for an event that can be caused only by one of the waiting processes. 
>
>The event in question is the execution of a signal() operation. 
>
>When such a state is reached, these processes are said to be **deadlocked**.

***

### 5.7 Classic Problems of Synchronization

**The Dining-Philosophers Problem**

>Consider five philosophers who spend their lives thinking and eating. The philosophers share a circular table surrounded by five chairs, each belonging to one philosopher. In the center of the table is a bowl of rice, and the table is laid with five single chopsticks.
>
>When a philosopher thinks, she does not interact with her colleagues. 
>
>From time to time, a philosopher gets hungry and tries to pick up the two chopsticks that are closest to her.

<img src=".\image-20201119093836077.png" alt="image-20201119093836077" style="zoom:50%;" />

- It is a simple representation of the need to allocate several resources among several processes in a deadlock-free and starvation-free manner.

- One simple solution is to represent each chopstick with a semaphore. 

  - A philosopher tries to grab a chopstick by executing a **wait()** operation on that semaphore. 

  - She releases her chopsticks by executing the signal() operation on the appropriate semaphores. Thus, the shared data are 

    ```c
    semaphore chopstick[5];
    
    do {
        wait(chopstick[i]);
        wait(chopstick[(i+1) % 5]);
        . . .
        /* eat for awhile */
        . . .
        signal(chopstick[i]);
        signal(chopstick[(i+1) % 5]);
        . . .
        /* think for awhile */
        . . .
    } while (true);
    ```

    where all the elements of chopstick are initialized to 1.

- Several possible remedies to the deadlock problem are replaced by:

  - Allow at most four philosophers to be sitting simultaneously at the table.
  - Allow a philosopher to pick up her chopsticks only if both chopsticks are available (to do this, she must pick them up in a critical section). 
  - Use an asymmetric solution—that is, an odd-numbered philosopher picks up first her left chopstick and then her right chopstick, whereas an even-numbered philosopher picks up her right chopstick and then her left chopstick.

***

### 5.8 Monitors

>Examples illustrate that various types of errors can be generated easily when programmers use semaphores incorrectly to solve the critical-section problem. 

- In this section, we describe one fundamental high-level synchronization construct—the **monitor** type.

```c
monitor monitor name
{
/* shared variable declarations */
    function P1 ( . . . ) {
    	. . .
    }
    function P2 ( . . . ) {
    	. . .
    }
        .
        .
        .
    function Pn ( . . . ) {
    	. . .
    }
    initialization_code ( . . . ) {
    	. . .
    }
}
```

The syntax of a monitor type is shown above.

**Monitor Usage**

> A monitor type is an ADT that includes a set of ***programmer-defined operations*** that are provided with mutual exclusion within the monitor.
>
> The monitor type also declares the ***variables*** whose values define the ***state*** of an instance of that type, along with the ***bodies of functions*** that operate on those variables. 

- However, the monitor construct, as defined so far, is not sufficiently powerful for modeling some synchronization schemes. 

  - For this purpose, we need to define additional synchronization mechanisms. 

  - These mechanisms are provided by the **condition** construct. 

  - A programmer who needs to write a tailor-made synchronization scheme can define one or more variables of type condition: 

    `condition x, y;`

<img src=".\11.png" style="zoom:33%;" />

- The only operations that can be invoked on a condition variable are wait() and signal(). 
  - The operation
    `x.wait();`
    means that the process invoking this operation is suspended until another process invokes
    `x.signal();`

***

**Dining-Philosophers Solution Using Monitors**

> This solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are available. 
>
> To code this solution, we need to distinguish among three states in which we may
> find a philosopher. For this purpose, we introduce the following data structure:
> `enum {THINKING, HUNGRY, EATING} state[5];`
>
> Philosopher i can set the variable state[i] = EATING only if her two neighbors are not eating: `(state[(i+4) % 5] != EATING) and (state[(i+1) % 5] != EATING).`

```c
monitor DiningPhilosophers
{
    enum {THINKING, HUNGRY, EATING} state[5];
    condition self[5];
    void pickup(int i) {
        state[i] = HUNGRY;
        test(i);
        if (state[i] != EATING)
        	self[i].wait();
    }
    void putdown(int i) {
        state[i] = THINKING;
        test((i + 4) % 5);
        test((i + 1) % 5);
    }
    void test(int i) {
        if ((state[(i + 4) % 5] != EATING) &&
        	(state[i] == HUNGRY) &&
        	(state[(i + 1) % 5] != EATING)) {
            state[i] = EATING;
            self[i].signal();
    	}	
    }
    initialization_code() {
        for (int i = 0; i < 5; i++)
        	state[i] = THINKING;
    }
}
```

- We also need to declare
  `condition self[5];`
  This allows philosopher i to delay herself when she is hungry but is unable to obtain the chopsticks she needs.

***

**Resuming Processes within a Monitor**

>How do we determine which of the suspended processes should be resumed next? One simple solution is to use a ***first-come, first-served (FCFS)*** ordering, so that the process that has been waiting the longest is resumed first. 
>
>In many circumstances, however, such a simple scheduling scheme is not adequate. 
>
>For this purpose, the **conditional-wait** construct can be used. This construct has the form
>
>`x.wait(c);`
>
>where c is an integer expression that is evaluated when the wait() operation is executed. The value of c, which is called a **priority number**, is then stored with the name of the process that is suspended. 
>
>When x.signal() is executed, the process with the smallest priority number is resumed next.

****

### 5.9 Synchronization Examples

**Synchronization in Windows**

***

## 6. CPU scheduling

### 6.1 Basic Concepts

**CPU–I/O Burst Cycle**

>The success of CPU scheduling depends on an observed property of processes: process execution consists of a **cycle** of CPU execution and I/O wait. 

- Processes alternate between these two states. Process execution begins with a CPU burst. That is followed by an I/O burst, which is followed by another CPU burst, then another I/O burst, and so on. 

<img src=".\12.png" style="zoom: 33%;" />

- The durations of CPU bursts have been measured extensively. 

<img src=".\13.png" style="zoom:33%;" />

***

**CPU Scheduler**

>Whenever the CPU becomes idle, the operating system must select one of the processes in the ready queue to be executed. The selection process is carried out by the **short-term scheduler**, or CPU scheduler. 

**Preemptive Scheduling**

- CPU-scheduling decisions may take place under the following four circumstances:
  - When a process switches from the running state to the waiting state.
  - When a process switches from the running state to the ready state (for example, when an interrupt occurs)
  - When a process switches from the waiting state to the ready state (for example, at completion of I/O)
  - When a process terminates

>When scheduling takes place only under circumstances 1 and 4, we say that the scheduling scheme is **nonpreemptive** or **cooperative**. Otherwise, it is **preemptive**. 

***

### 6.2 Scheduling Criteria





***

### 6.3 Scheduling Algorithms

**First-Come, First-Served Scheduling**

> By far the simplest CPU-scheduling algorithm is the first-come, first-served (**FCFS**) scheduling algorithm. 
>
> With this scheme, the process that requests the CPU first is allocated the CPU first. 

- On the negative side, the average waiting time under the FCFS policy is often quite long. 

**Shortest-Job-First Scheduling**

>A differentapproachto CPU schedulingisthe shortest-job-first (SJF)scheduling algorithm. This algorithm associates with each process the length of the process’s next CPU burst. 

- With short-term scheduling, there is no way to know the length of the next CPU burst. 
  - One approach to this problem is to try to approximate SJF scheduling. 
  - We may not know the length of the next CPU burst, but we may be able to predict its value. We expect that the next CPU burst will be similar in length to the previous ones. 
  - The next CPU burst is generally predicted as an exponential average of the measured lengths of previous CPU bursts. 
  - We can define the exponential average with the following formula. 